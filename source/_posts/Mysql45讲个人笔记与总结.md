---
0title: Mysql45讲个人笔记与总结
top: false
cover: false
toc: true
mathjax: true
categories:
  - mysql
tags:
  - mysql
date: 2021-11-13 19:43:52
password:
summary:
---

# Mysql45

# 一、基础架构，SQL语句如何执行

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111131946742.png)

## 1.1 Server层

Server层：查询缓存、分析器、优化器、执行器等 以及所有内置的函数（eg:日期、时间、数学和加密函数等）所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

### 1.1.1 连接器

​			客户端连接到服务端，获取到==权限==等信息，然后在连接的有效时长内(==interactive_timeout==和==wait_timeout==参数控制， 5.7版本会断开可以自动重连)对sql进行处理。

* connect-timeout：连接过程中的等待时间
* wait-timeout：连接完成后，使用过程中的等待时间
* interactive-timeout：交换式……

`show processlist`

```mysql
# 通过命令可以去查看连接状态
show processlist
show variables like 'wait_timeout';
```

**注意：**

> 连接器会到权限表中查询拥有的权限，之后这个连接练得权限判断逻辑，都依赖于读到的权限，并且，连接在建立之后如果修改权限也不影响已经存在的连接，只有连接新建的时候，才会生效

**长链接与短连接**

> **长链接与短连接**是一种“行为”，比如连接完，执行一个查询，就断开，这是短连接；执行一个查询，不断开，下次查询还用这个连接，持续使用，就是长链接
>
> 换句话说，在不用连接池的时候，每次查询都要获取连接，查询完成之后调用close方法，这就是短链接，使用连接池之后，因为连接都被放入容器中，每次用完就放进去，而不是close，这种就是长连接

**mysql_reset_connection** 影响的会话状态相关的信息

> mysql_reset_connection()影响以下与会话状态相关的信息： ·回滚活跃事务并重新设置自动提交模式 ·释放所有的表锁 ·关闭或删除所有的临时表 ·重新初始化会话的系统变量值 ·丢失用户定义的变量设置 ·释放prepared语句 ·关闭handler变量 ·将last_insert_id()的值设置为0 ·释放get_lock()获取的锁 ·清空通过mysql_bind_param()调用定义的当前查询属性 返回值： 0表示成功，非0表示发生了错误。

#### FQA

**如何解决长链接所导致的内存占用过大MySQL异常重启问题（OOM）？**

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

2. 首先会判断查询缓存是否开启，如果已经开启，会判断sql是select还是update/insert/delete，对于select，尝试去查询缓存，如果命中缓存直接返回数据给客户端， 如果缓存没有命中，或者没有开启缓存， 会进入到下一步分析器。

### 1.1.2 查询缓存

> 查询缓存：mysql拿到一个查询后，先查询缓存，（缓存保存形式KV（K为查询的语句，V为查询的结果））

#### FQA

**为什么建议关闭缓存？**

> 失效频繁，而且对于更新压力大的数据库，命中率非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。==mysql8.0版本取消了查询缓存的整个功能模块。==

```mysql
# 关闭查询缓存
show variables like '%query_cache_type%';
set GLOBAL query_cache_type='OFF';
```

### 1.1.3 分析器

> 分析器进行语法分析(根据词法分析的结果和语法规则，判断是否满足Mysql语法)、词法分析，检查sql的语法顺序等得到==解析树==， 然后预处理器对解析树进一步分析，==验证数据表、字段是否存在==，通关之后sql进入下一步优化器，

### 1.1.4 优化器

> 优化器对sql执行计划分析，决定使用哪个索引；在一个语句有多表关联（join）时，决定各个表的连接顺序，得到最终执行计划，得到优化后的执行计划之后交给执行器。

执行器调用存储引擎api执行sql，得到响应结果， 将结果返回给客户端，如果缓存是开启状态， 会更新缓存。

### 1.1.5 执行器

> 执行器在执行的时候会先判断对于表T有没有查询权限，如果没有返回没有权限的错误(在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

**precheck验证权限**

> 权限验证不仅仅在执行器这部分会做，在分析器之后，也就是知道了该语句要“干什么”之后，也会先做一次权限验证。叫做precheck。而precheck是无法对运行时涉及到的表进行权限验证的，比如使用了触发器的情况。因此在执行器这里也要做一次执行时的权限验证。

**执行流程**

> 1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
> 2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
> 3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

FAQ

## 1.2 存储引擎层

负责数据的存储和提取，架构模式位==插件式==的支持InnoDB（默认）、MyISAM（不支持事务）、Memory（针对具体表，而非数据库）

InnoDB、MyISAM、Memory比较：：
InnoDB：支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（银行），
要求实现并发控制（售票），选择InnoDB。如果需要频繁更新、删除操作的数据库也选择InnoDB。因为支持
事务的提交和回滚
MyISAM：插入数据块、空间和内存使用比较低。如果表主要用于插入新记录和读出记录，选择MyISAM能实现处理高效率。
如果应用的完整性、并发性要求比较低，也可使用。
Memory：所有数据存在内存中，数据处理速度快，但不安全。如果需要很快的读写速度，对数据的安全性要求低，可选择Memory
它对表的大小有要求，不能建立太大的表。

# 二、日志系统，更新语句如何执行

**关于缓存**

​		对于MySQL8.0之前的版本，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这就是一般不建议使用查询缓存的原因。

## 2.1 redo log 重做日志

> 客户端执行DDL语句（create）/DML语句（insert，update，delete）/DCL语句（grant，revoke），数据库服务端执行的时候会涉及到 redo log（重做日志） 和 binlog（归档日志） 两个日志文件的更新。

**问题引出：**

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 ==IO 成本==、查找成本都很高。

**关于IO成本**

> IO成本就是寻址时间和上下文切换所需要的时间，最主要是用户态和内核态的上下文切换。我们知道用户态是无法直接访问磁盘等硬件上的数据的，只能通过操作系统去调内核态的接口，用内核态的线程去访问。 这里的上下文切换指的是同进程的线程上下文切换，所谓上下文就是线程运行需要的环境信息。 首先，用户态线程需要一些中间计算结果保存CPU寄存器，保存CPU指令的地址到程序计数器（执行顺序保证），还要保存栈的信息等一些线程私有的信息。 然后切换到内核态的线程执行，就需要把线程的私有信息从寄存器，程序计数器里读出来，然后执行读磁盘上的数据。读完后返回，又要把线程的信息写进寄存器和程序计数器。 切换到用户态后，用户态线程又要读之前保存的线程执行的环境信息出来，恢复执行。这个过程主要是消耗时间资源。 --来自《Linux性能优化实战》里的知识 SQL执行前优化器对SQL进行优化，这个过程还需要占用CPU资源

**关于随机IO**

> 随机 IO。 避免每次更新都要通过磁盘随机 IO 定位到记录位置。 将改动以顺序 IO 写到 redo log。可以使用组提交来批量更新。 类比，其实很多组件都是这么做的。 比如 Redis 的 Pipeline。以减少网络 IO。批量请求。

### 2.1.1 WAL技术

> MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。
>
> 详细的说：先写redo log到log buffer，具体内容就是针对哪个表空间的哪些页面做了哪些修改，然后log buffer中的日志内容会在某些时候写到redo日志文件中，比如事务提交时。至于为什么写redo日志会比刷新内存中的数据页到磁盘快，是因为服务器在启动时就已经给redo日志文件分配好了一块物理上连续的磁盘空间，每次写redo日志都是往文件中追加写，并没有寻址的过程。而修改过的数据页要刷新到磁盘的话，可能对应的磁盘空间并不是物理连续的，找起来费劲

​			当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并==更新内存==，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲（根据innodb_flush_log_at_trx_commit来决定）的时候做

**什么是更新内存**

> 更新内存的意思是先要把这一行记录从磁盘加载到内存中(buffer_pool),然后在内存中更新这个值。不会立即把最新值刷新到磁盘。
>
> 当需要更新的数据页在内存中时，就会直接更新内存中的数据页；不在内存中时，在可以使用 change buffer 的情况下，就会将更新操作记录到 change buffer 中，并将这些操作记录到 redo log 中；

---

### 2.1.3 redo log 图

> redo log 是循环写的，空间固定会用完，之后就去写磁盘，有刷页机制；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
>
> redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111210009122.png)

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

**落盘机制**

innodb_flush_log_at_trx_commit 变量控制日志缓冲区的内容如何写入和刷新到磁盘。innodb_flush_log_at_timeout 变量控制日志刷新频率(1s由log buffer刷盘一次）

> 落盘机制可以通过innodb_flush_log_at_trx_commit参数来控制：
>    当设置为1的时候，事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file on disk中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。
>    当设置为0的时候，事务提交时不会将log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到log file on disk中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。
>    当设置为2的时候，每次提交都仅写入到os buffer，然后是每秒调用fsync()将os buffer中的日志写入到log file on disk。

1. 后台线程定期会刷脏页
2. 清理LRU链表时会顺带刷脏页
3. redoLog写满会强制刷
4. 数据库关闭时会将所有脏页刷回磁盘
5. 脏页数量过多（默认占缓冲池75%）时，会强制刷

### 2.1.3 crash-safe

> redo log 是 InnoDB引擎所特有的，所以我们如果再使用InnoDB引擎创建表时，如果数据库发生异常重启，之前提交的记录都不会丢失。 InnoDB正因为有了 redo log(重做日志)，才有了 crash-safe 的能力（即使mysql服务宕机，也不会丢失数据的能力）。

数据库重启了，内存中的数据页没有同步到磁盘中，可以通过redo log日志恢复

## 2.2 binlog 归档日志

binlog（归档日志）是Server 层自己的日志。

 如何查看 binlog： https://zhuanlan.zhihu.com/p/33504555 

1. 搜索 log 名称：show variables like '%log_bin%'; 
2.  查看 log 内容：show binlog events in 'binlog.000009’ 或 使用命令行工具：mysqlbinlog binlog.000009

**关于binlog**

> binlog日志格式binlog日志有两种格式，分别为STATMENT、ROW，运行模式有三种，分别为STATMENT、ROW和MIXED。 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。 STATMENT 基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。 * 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能； * 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。 ROW 基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。 * 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题； * 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨 MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlogredo log

**为什么会有两份日志？**

> 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。
>
> binlog还不能去掉。 一个原因是，redolog只有InnoDB有，别的引擎没有。 另一个原因是，redolog是循环写的，不持久保存，binlog的“归档”这个功能，redolog是不具备的。 只有两份日记才有crash-safe功能 为什么binlog不能做到crash-safe？ 假如只有binlog，有可能先提交事务再写binlog，有可能事务提交数据更新之后数据库崩了，还没来得及写binlog。我们都知道binlog一般用来做数据库的主从复制或恢复数据库，这样就导致主从数据库不一致或者无法恢复数据库了。同样即使先写binlog再提交事务更新数据库，还是有可能写binlog成功之后数据库崩掉而导致数据库更新失败，这样也会导致主从数据库不一致或者无法恢复数据库。所以只有binlog做不到crash-safe。为了支持crash-safe，需要redolog，而且为了保证逻辑一致，事务提交需要两个阶段：prepare阶段和commit阶段。写redolog并落入磁盘(prepare状态)-->写binlog-->commit。commit的时候是不会落盘的。

**binlog为什么没有crash_safe的能力呢？**

> 不考虑mysql现有的实现，假如现在重新设计mysql，只用一个binlog是否可以实现cash_safe能力呢？答案是可以的，只不过binlog中也要加入checkpoint，数据库故障重启后，binlog checkpoint之后的sql都重放一遍。但是这样做让binlog耦合的功能太多。
>
> 并且写入方式的问题，binlog是追加写，crash时不能判定binlog中哪些内容是已经写入到磁盘，哪些还没被写入。而redolog是循环写，从check point到write pos间的内容都是未写入到磁盘的。

**binlog 与redolog的不同**

> 1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
> 2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
> 3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
> 4. 两者在更新方面也不一样redo log 分为 redo log buffer 和 redo log file，buffer 到 file 是通过 os buffer 写入，写入机制分别为【延迟写】: 每秒从 redo log buffer 写入到 os buffer 和 redo log file， 【实时写，实时刷】: 即无延时实时写入，【实时写，延迟刷】: 每次写入到 os buffer 后每秒刷到 redo log file， binlog是在事务提交后一次性写入

REDO的写盘时间会直接影响系统吞吐，显而易见，REDO的数据量要尽量少。其次，系统崩溃总是发生在始料未及的时候，当重启重放REDO时，系统并不知道哪些REDO对应的Page已经落盘，因此REDO的重放必须可重入，即REDO操作要保证幂等。最后，为了便于通过并发重放的方式加快重启恢复速度，REDO应该是基于Page的，即一个REDO只涉及一个Page的修改。 熟悉的读者会发现，数据量小是Logical Logging的优点，而幂等以及基于Page正是Physical Logging的优点，因此InnoDB采取了一种称为Physiological Logging的方式，来兼得二者的优势。所谓Physiological Logging，就是以Page为单位，但在Page内以逻辑的方式记录。举个例子，MLOG_REC_UPDATE_IN_PLACE类型的REDO中记录了对Page中一个Record的修改，方法如下： （Page ID，Record Offset，(Filed 1, Value 1) ... (Filed i, Value i) ... ) 其中，PageID指定要操作的Page页，Record Offset记录了Record在Page内的偏移位置，后面的Field数组，记录了需要修改的Field以及修改后的Value。

---

**执行器与InnoDB引擎执行update语句内部流程**

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的==数据页==本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。<!--操作系统层面读取磁盘的最小单位本来就是页，内存页、cache页和磁盘页都是一样大小，一般是4KB，但是Innodb使用B+树存储数据时，一个树节点即一次磁盘IO到内存是16KB，4倍关系。-->
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行==新数据==。<!--写在了内存中，innodb中的内存模型中有一个buffer pool的结构。innodb的存储结构，它分为内存结构和磁盘结构。-->
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。<!--1.binlog只有在commit的时候才会写入； 2.当prepare log 写入成功且binglog写入成功后发生crash，在mysql启动时候，会自动commit这个事物； 3.当prepare log写入成功，binlog写入失败，此时发生crash，mysql启动会自动回滚掉这个事物。即Binlog如果已经写入磁盘，那么redo log是prepare, 且binlog已经完整了，这时候崩溃恢复过程会认可这个事务，提交掉。 达到了“用binlog恢复的库跟原库逻辑相同” 这个要求-->

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111211530363.png)

**数据恢复步骤**

> 恢复的时候的大致步骤可能如下，摘取下来仅供做设计思想的参考：
>
>  **Step1.** 按顺序扫描redolog，如果redolog中的事务既有prepare标识，又有commit标识，就直接提交（先将磁盘页读入内存，再用 redo更新形成 dirty page，然后再刷盘） **Step2** . 如果redolog事务只有prepare标识，没有commit标识，则说明当前事务在commit阶段crash了，binlog中当前事务是否完整未可知，此时拿着redolog中当前事务的XID（redolog和binlog中事务落盘的标识），去查看binlog中是否存在此XID a. 如果binlog中有当前事务的XID，则提交事务（复制redolog disk中的数据页到磁盘数据页） b. 如果binlog中没有当前事务的XID，则回滚事务（使用undolog来删除redolog中的对应事务）

### 2.2.1 两阶段提交

> 将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

**怎样让数据库恢复到半个月内任意一秒的状态？**

通过定期的整库备份加上binlog的操作回放可以保证数据的安全性。因为binlog记录的是数据的逻辑操作（原始sql语句），而redolog是数据的物理操作日志，并且非innodb的引擎没有redolog。因此回放的时候回使用binlog进行回放

例如：某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

* 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
* 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

**注：**在重放binlog之前，需要将binlog中误删除的那个位置前的操作给删除掉，不然还是会执行误删除操作。等于前面的所有操作都白费了，也可以指定重放的位置，重放到误删除操作之前的position。

---

**为什么要两阶段提交？**

> 因为从 “两阶段提交”的执行流程看，“ binlog 成功，redo log prepare 失败”的场景， redo log 和 binlog 还是不一致的。 真正的“两阶段提交” 是指对 redo log 进行“两阶段提交”：先 prepare，再commit。 数据库 crash-重启后，会对记录对redo log 进行check
>
> 1. 如果 redo log 已经commit，则视为有效。
>
> 2. 如果 redo log prepare 但未commit，则check对应的bin log记录是否记录成功。 
>    1. bin log记录成功则将该prepare状态的redo log视为有效 
>    2. bin log记录不成功则将该prepare状态的redo log视为无效

本质上是因为 redo log 负责事务； binlog负责归档恢复； 各司其职，相互配合，才提供(保证)了现有功能的完整性； 现在 你非要破坏 其中一个log，完了，还妄想保证上述的功能，怎么可能呢？ 除非你从根本上 改写binlog，合并redo log 和 binlog 的 职责和功能！

redolog和binlog具有关联行，在恢复数据时，redolog用于恢复主机故障时的未更新的物理数据，binlog用于备份操作。每个阶段的log操作都是记录在磁盘的，在恢复数据时，redolog 状态为commit则说明binlog也成功，直接恢复数据；如果redolog是prepare，则需要查询对应的binlog事务是否成功，决定是回滚还是执行。

简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 2.2.2 小结

> redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。<!--innodb_flush_log_at_trx_commit={0|1|2} # 指定何时将事务日志刷到磁盘，默认为1。 0表示每秒将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。 1表示每事务提交都将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。 2表示每事务提交都将"log buffer"同步到"os buffer"但每秒才从"os buffer"刷到磁盘日志文件中。-->

> sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

## 2.3 FAQ

**全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？**

一天一备和一周一备： - 好处：最长恢复时间更短。 一天一备——全备+这一天0点到当前时间的binlog (如果每次0点全备) 一周一备——全备+周一到当前时间的binlog (如果每周周一0点全备) - 代价：一天一备，频繁全量备份，需要消耗大量存储空间

在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。

一周一备最坏情况就要应用一周的 binlog 了。

系统的对应指标就是  RTO（恢复目标时间）。

当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间增加磁盘压力，所以这个 RTO 是成本换来的，就需要你根据业务重要性来评估了。

---

**数据库备份策略**

备份就是救命药加后悔药，灾难发生的时候备份能救命，出现错误的时候备份能后悔。事情都有两面性，没有谁比谁好，只有谁比谁合适，完全看业务情况和需求而定。一天一备恢复时间更短，binlog更少，救命时候更快，但是后悔时间更短，而一周一备正好相反。我自己的备份策略是设置一个16小时延迟复制的从库，充当后悔药，恢复时间也较快。再两天一个全备库和binlog，作为救命药,最后时刻用。这样就比较兼顾了。

---

**为什么先写日志后写磁盘还可以查到数据呢？**

因为数据已经在内存中了，不会去查找磁盘了

如果update到内存后，redolog/binlog写入之前，这个时刻【如果】有其他客户端B能读到的话（比如 10 更新为20了），从客户端来的角度来看的话，这个20是已经更新完成的了。然而如果redolog/binlog写入之前崩溃了，恢复时，这个更新肯定是要丢弃的（因为日志还没commit）；这时候之前的读就是有问题的。所以个人认为在commit之前，哪怕是更新到内存了，其他读取理应是看不到这个尚未commit的数据的（和事务隔离级别有关？）

---

**由于脏页导致的普通select超过30s**

所谓刷脏就是由于内存页和磁盘数据不一致导致了该内存页是“脏页”，将内存页数据刷到磁盘的操作称为“刷脏”。刷脏是为了避免产生“脏页”，主要是因为MySQL更新先写redo log再定期批量刷到磁盘的，这就导致内存页的数据和磁盘数据不一致，为了搞清楚为什么“刷脏”会导致慢查，我们先分析下redo log再哪些场景会刷到磁盘。
场景1：redo log写满了，此时MySQL会停止所有更新操作，把脏页刷到磁盘
场景2：系统内存不足，需要将脏页淘汰，此时会把脏页刷到磁盘
场景3：系统空闲时，MySQL定期将脏页刷到磁盘

可以想到，在场景1和2都会导致慢查的产生，根据文章提到的，redo log是可以循环写的，那么即使写满了应该也不会停止所有更新操作吧，其实是会的，文中有句话“粉板写满了，掌柜只能停下手中的活，把粉板的一部分赊账记录更新到账本中，把这些记录从粉板删除，为粉板腾出新的空间”，这就意味着写满后是会阻塞一段时间的。

那么问题来了，innodb存储引擎的刷脏策略是怎么样的呢？通常而言会有两种策略：全量（sharp checkpoint）和部分（fuzzy checkpoint）。全量刷脏发生在关闭数据库时，部分刷脏发生在运行时。部分刷脏又分为定期刷脏、最近最少使用刷脏、异步/同步刷脏、脏页过多刷脏。

---

**关于数据页**

MySQL的记录是以“页”为单位存取的，默认大小16K。也就是说，你要访问磁盘中一个记录，不会只读这个记录，而会把它所在的16K数据一起读入内

---

**如果两种log都写完成了，但是redolog没有写磁盘，物理机挂了，redolog在内存中就丢失了吧，再启动跟磁盘中的binlog就不一致，后期恢复数据的时候会有问题吗**

需要讲trx_commit设置成1

---

**redo log为什么要设计成环形的？环形缓冲区的设计又有什么好？**

可以避免新建、删除文件带来的抖动

# 三、事物隔离

## 3.1 隔离性与隔离级别 

## 3.2 事物的ACID属性

* **Atomicity（原子性）：**

    一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 

* **Consistency（一致性）：**

  在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。

*  **Isolation（隔离性）：**

  数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 

* **Durability（持久性）：**

  事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

> 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。

* **脏读:** 读到其他事务未提交的数据； 

* **不可重复读：**前后读取的记录内容不一致；

* **幻读：**前后读取的记录数量不一致。幻读也成幻行，指的是第一次查询和第二次查询返回的行集不一致。所以删除了一行数据也会导致幻读

## 3.3 事物的隔离级别

1. **读未提交RU：**一个事务还没提交时，它做的变更就能被别的事务看到。会造成“脏读”，“幻读”，“不可重复读取”。 
2. **读提交RC：**一个事务提交之后，它做的变更才会被其他事务看到。避免了“脏读”，“但不能避免""幻读“和”不可重复读取”。(是大多主流数据库默认的事务等级)
3. **可重复读RR：**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。避免了“脏读“和”不可重复读取“的情况，但不能避免“幻读”。（带来了更多的性能损失）
4. **串行化Seria：**顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。（最严格级别，事务串行执行，资源消耗最大）

**注意：**

> 在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，要记得将 MySQL 的隔离级别设置为“读提交”
>
> 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。
>
> 5.7版本就是@@tx_isolation，8.0以上是transaction_isolation

## 3.4 视图

数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

1. **RC级别:**   MVCC视图会在每一个语句前创建一个，所以在RC级别下，一个事务是可以看到另外一个事务已经提交的内容、因为它在每一次查询之前都会重新给予最新的数据创建一个新的MVCC视图。 

2. **RR级别:** MVCC视图在，视图是在第一个Select语句执行时创建的吧？，这个视图会一直使用，直到该事务结束。 这里要注意不同的隔离级别他们的一致性事务视图创建的时间点是不同的。 

   事务最开始是update语句时，这个时候还没创建视图，当事务询查到第一条查询语句才开始创建视图

3. **RU：**没有视图的概念，直接返回记录上的最新值

4. **串行化：** 直接用加锁的方式来避免并行访问。

## 3.5 事物隔离的实现

> 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。<!--意思就是除了记录变更记录，还会记录一条变更相反的回滚操作记录，前者记录在redo log，后者记录在undo log-->

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111231403770.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小

## 3.7 事务的启动方式

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。一致性视图是在执行第一个快照读语句时创建的；
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。
3.  start transaction with consistent snapshot; 如果你想要马上启动一个事务，可以使用 `start transaction with consistent snapshot `这个命令。  一致性视图是在执行 start transaction with consistent snapshot 时创建的。
4. commit work and chain 将提交和开启事务用一条语句完成可以弥补，减少交互，来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

> 很多语言的第三方库都是使用连接池维持可复用的长连接来保持与mysql的链接，需注意，长连接并不意味着长事务，需要判断是否将autocommit设置成了0.

建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。

如果纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。

## 3.8 长事务查询

information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

## FAQ

**回滚日志何时删除?**

> 在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。
>
> 回归日志生命周期 在开始时创建 提交后不一定删除 只有在提交后且没有比当前更早的事务时 回滚日志才会被删除 所以尽量不要使用长事务

**为什么不要使用长事务?**

 长事务意味着系统里面会存在很老的事务视图。 由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。

**如何避免长事务**

首先，从应用开发端来看：

1. 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。

   ```bash
   一、查询日志开启 
   方法一： 　　
   #设置路径
   mysql>set global general_log_file='/tmp/general.lg';  　
   # 开启general log模式 
   mysql>set global general_log=on; 　　
   # 关闭general log模式 命令行设置即可,无需重启 在general log模式开启过程中，所有对数据库的操作都将被记录 general.log 文件 
   mysql>set global general_log=off; 
   方法二： 也可以将日志记录在表中 
   set global log_output='table' 运行后,可以在mysql数据库下查找 general_log表 
   二、查询日志关闭 查看是否是开启状态： 
   mysql> show global variables like '%general%'; 
   #  关闭查询日志
   mysql> set global general_log = off; 
   ```

   

2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。

3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

其次，从数据库端来看：

1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
2. Percona 的 pt-kill 这个工具不错，推荐使用；
3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

# 四、深入浅出索引一

> 索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本 500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。

## 4.1 B+树

哈希表能快速找到数据，但是不支持范围查找，有序数组支持范围查找，但是不支持随机插入，B+树俩者都支持。

### 4.1.1 MySQL的存储结构

#### 4.1.1.1 表存储结构

单位：表>段>区>页>行

在数据库中， 不论读一行，还是读多行，都是将这些行所在的页进行加载。也就是说存储空间的基本单位是页。

一个页就是一棵树B+树的节点，数据库I/O操作的最小单位是页，与数据库相关的内容都会存储在页的结构里。

#### 4.1.1.2 B+树索引结构

在一棵B+树中，每个节点为都是一个页，每次新建节点的时候，就会申请一个页空间

同一层的节点为之间，通过页的结构构成了一个双向链表

非叶子节点为，包括了多个索引行，每个索引行里存储索引键和指向下一层页面的指针

叶子节点为，存储了关键字和行记录，在节点内部(也就是页结构的内部)记录之间是一个单向的表

#### 4.1.1.3 B+树页节点结构

有以下几个特点

将所有的记录分成几个组， 每组会存储多条记录，

页目录存储的是槽(slot)，槽相当于分组记录的索引，每个槽指针指向了不同组的最后一个记录

我们通过槽定位到组，再查看组中的记录

页的主要作用是存储记录，在页中记录以单链表的形式进行存储。

单链表优点是插入、删除方便，缺点是检索效率不高，最坏的情况要遍历链表所有的节点。因此页目录中提供了二分查找的方式，来提高记录的检索效率。

#### 4.1.1.4 B+树的检索过程

我们再来看下B+树的检索过程

从B+树的根开始，逐层找到叶子节点。

找到叶子节点为对应的数据页，将数据叶加载到内存中，通过页目录的槽采用二分查找的方式先找到一个粗略的记录分组。

在分组中通过链表遍历的方式进行记录的查找。

#### 4.1.1.5 为什么要用B+树索引

数据库访问数据要通过页，一个页就是一个B+树节点，访问一个节点相当于一次I/O操作，所以越快能找到节点，查找性能越好。

B+树的特点就是够矮够胖，能有效地减少访问节点次数从而提高性能。

下面，我们来对比一个二叉树、多叉树、B树和B+树。

### 4.1.2 二叉树

二叉树是一种二分查找树，有很好的查找性能，相当于二分查找。

但是当N比较大的时候，树的深度比较高。数据查询的时间主要依赖于磁盘IO的次数，二叉树深度越大，查找的次数越多，性能越差。

最坏的情况是退化成了链表，如下图

为了让二叉树不至于退化成链表，人们发明了AVL树(平衡二叉搜索树)：任何结点的左子树和右子树高度最多相差1

### 4.1.3 多叉树

多叉树就是节点可以是M个，能有效地减少高度，高度变小后，节点变少I/O自然少，性能比二叉树好了

### 4.1.4 B树

B树简单地说就是多叉树，每个叶子会存储数据，和指向下一个节点的指针。

例如要查找9，步骤如下

我们与根节点的关键字 (17，35)进行比较，9 小于 17 那么得到指针 P1；

按照指针 P1 找到磁盘块 2，关键字为(8，12)，因为 9 在 8 和 12 之间，所以我们得到指针 P2；

按照指针 P2 找到磁盘块 6，关键字为(9，10)，然后我们找到了关键字 9。

### 4.1.5 B+树

B+树是B树的改进，简单地说是：只有叶子节点才存数据，非叶子节点是存储的指针；所有叶子节点构成一个有序链表

例如要查找关键字16，步骤如下

与根节点的关键字 (1，18，35) 进行比较，16 在 1 和 18 之间，得到指针 P1(指向磁盘块 2)

找到磁盘块 2，关键字为(1，8，14)，因为 16 大于 14，所以得到指针 P3(指向磁盘块 7)

找到磁盘块 7，关键字为(14，16，17)，然后我们找到了关键字 16，所以可以找到关键字 16 所对应的数据。

### 4.1.6 B+树与B树的不同：

B+树非叶子节点不存在数据只存索引，B树非叶子节点存储数据

B+树使用双向链表串连所有叶子节点，区间查询效率更高，因为所有数据都在B+树的叶子节点，但是B树则需要通过中序遍历才能完成查询范围的查找。

B+树每次都必须查询到叶子节点才能找到数据，而B树查询的数据可能不在叶子节点，也可能在，这样就会造成查询的效率的不稳定

B+树查询效率更高，因为B+树矮更胖，高度小，查询产生的I/O最少。

这就是MySQL使用B+树的原因，就是这么简单！

## 4.2 InnoDB 的索引模型

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

对于Innodb对应的N叉树大小，以InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。

MySql默认一个节点的长度为16K，一个整数（bigint）字段索引的长度为 8B,另外每个索引还跟着6B的指向其子树的指针；所以16K/14B ≈ 1170 参见https://blog.csdn.net/weixin_35871519/article/details/113303881

https://github.com/jeremycole/innodb_diagrams/blob/master/images/InnoDB_Structures.pdf

**关于 InnoDB 的表结构：**

1. 在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树。

2. 执行查询的效率，使用主键索引 > 使用非主键索引 > 不使用索引。 

3. 如果不使用索引进行查询，则从主索引 B+ 树的叶子节点进行遍历。

**每一个索引在 InnoDB 里面对应一棵 B+ 树。**

**例如：**

```mysql
create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;
insert into t(id,k) values(100,1),(200,2),(300,3),(500,5),(600,6);
```

![InnoDB 的索引组织结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111231611454.png)

> 二级索引的叶子节点存的就是主键的值，不是引用！根据这个值进行回表，再在聚簇索引里面直接从叶子节点里面拿到值，不需要进行一次IO去磁盘找。为啥存值不存引用（数据在磁盘的地址指针）？因为一旦发生页分裂或者页合并，就得去维护这个地址指针，更麻烦。

---

**基于主键索引和普通索引的查询有什么区别？**

主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

---

## 4.3 索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。

除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。

数据页参见https://www.cnblogs.com/zhuchangwu/p/14041410.html

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。

**适合业务字段做主键的场景**

1. 只有一个索引；

2. 该索引必须是唯一索引。  

这就是典型的 KV 场景。由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。

这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。

## FAQ

**重建索引问题**

如果你要重建索引 k，你的两个 SQL 语句可以这么写：

`alter table T drop index k;`

`alter table T add index(k);`

如果你要重建主键索引，也可以这么写：

`alter table T drop primary key;`

`alter table T add primary key(id);`

对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？

1. 直接删掉主键索引是不好的，它会使得所有的二级索引都失效，并且会用ROWID来作主键索引；
2. 看到mysql官方文档写了三种措施，第一个是整个数据库迁移，先dump出来再重建表（这个一般只适合离线的业务来做）；第二个是用空的alter操作，比如ALTER TABLE t1 ENGINE = InnoDB;这样子就会原地重建表结构（真的吗？）；第三个是用repaire table，不过这个是由存储引擎决定支不支持的（innodb就不行）。

**注意**

> 记录日志的表最好是分区表，历史数据清理可以直接drop分区	

---

**“N叉树”的N值在MySQL中是可以被人工调整的么？**

1， 通过改变key值来调整
N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
2， 改变页的大小
页越大，一页存放的索引就越多，N就越大。

数据页调整后，如果数据页太小层数会太深，数据页太大，加载到内存的时间和单个数据页查询时间会提高，需要达到平衡才行。

**没有主键的表，有一个普通索引。怎么回表？**

https://dev.mysql.com/doc/refman/5.6/en/innodb-index-types.html
5.6 文档
先找非空唯一索引；
如果没有，再用rowid 

**innodb B+树主键索引的叶子节点存的是什么?**

B+树的叶子节点是page （页），一个页里面可以存多个行

 因为存的不可能是“页” 这一逻辑概念 只能说这个叶结点大小等于innoDB里设置的页大小 或者说这个叶结点其实就是“页” 但存的是什么 那当然是数据 什么数据 当然是表中的行数据

**普通索引，为什么要用回表，直接指向数据行，数据页不行吗？**

第一、主键索引和普通索引，实际指向的都是一个页地址，在每个页中，主键索引页每行的value存的是实际数据，普通索引页每行的value是主键id。
第二、如果普通索引页每行的value是主键索引页地址，那么在发生页分裂、页合并、主键索引重建的时候，需要遍历所有的普通索引，查找涉及到的普通索引key进行更新。
第三、可以理解为：使用id值，相当于对主键索引与普通索引进行了解耦，主键索引页的变更不会影响到其他的普通索引。
第四、关于解耦带来的好处，与查询上带来的性能损失之间的定量分析，希望有其他同学补充
第五、可能还有其他的原因，欢迎其他同学补充

**1、如果插入的数据是在主键树叶子结点的中间，后面的所有页如果都是满的状态，是不是会造成后面的每一页都会去进行页分裂操作，直到最后一个页申请新页移过去最后一个值**

只会分裂它要写入的那个页面。每个页面之间是用指针串的，改指针就好了，不需要“后面的全部挪动

在Page 数据结构中，记录 Page 的头信息的File Header 字段中有 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 字段，通过这两个字段，来确定该页的上一页和下一页，实际上所有页通过两个字段可以形成一条双向链表的

**2、还有之前看到过说是插入数据如果是在某个数据满了页的首尾，为了减少数据移动和页分裂，会先去前后两个页看看是否满了，如果没满会先将数据放到前后两个页上，不知道是不是有这种情况**

对，减为了增加空间利用率

**为什么现在一般自增索引都设置为bigint**

因为现在很多业务插入数据很凶残，容易超过int 上限，实际上是建议设置bigint unsigned



**想问如果主键开始是int，自增不够了，改成bigint，内部该怎么处理，是新建表，还是要分裂**？



---

**索引只能定位到page，page内部怎么去定位行数据**

知识点:Page directory ,内存中利用二分查找

https://www.cnblogs.com/bdsir/p/8745553.html感觉这个很清楚

https://blog.csdn.net/cy973071263/article/details/104512020

---

**非聚集索引上为啥叶子节点的value为什么不是地址，这样可以直接定位到整条数据，而不用再次对整棵树进行查询**

这个叫作“堆组织表”，MyISAM就是这样的，各有利弊。你想一下如果修改了数据的位置的情况，InnoDB这种模式是不是就方便些，对于主键索引页分裂的场景， 就可能会导致主键记录的地址发生变化， 这时候需要更新每一个索引上面对主键记录地址的引用

---

**整张表的数据其实就是存在主键索引中的**

---

**什么情况下创建索引才有意义？有哪些限制？比如字段长度**

 有这个索引带来的查询收益，大于维护索引的代价，就该建😄 对于可能变成大表的表，实际上如果不建索引会导致全表扫描，这个索引就是必须的。

---

**如何查看索引占用多少空间？**

可以估算出来的，根据表的行数和索引的定义。

---

**查看索引数的结构，比如多少个层，多少节点？**

跟上一个一样。 如果要精确的，就要解数据文件，这个工具可以看看https://github.com/jeremycole/innodb_diagrams

---

**如何查看索引的利用率。比如我创建了一个索引，是否可以有记录这个索引被调用了多少次？**

 performance_schema.table_io_waits_summary_by_index_usage能看到一些信息

---

**关于innodb中自增索引的插入问题**

按照传统B+树的插入规则，即使是自增插入，当一个数据页满的时候，也是会引起页分裂的。但是innodb在这一块做了优化，即判断如果是自增插入且当前页已满的情况下，不改变原有页的结构，而是将新的数据放到一个新页中。
在innodb的实现中，为每个索引页面维护了一个上次插入的位置，以及上次的插入是递增/递减的标识。根据这些信息，innodb能够判断出新插入到页面中的记录，是否仍旧满足递增/递减的约束，若满足约束，则采用优化后的分裂策略；若不满足约束，则退回到传统的分裂策略。

---

**在插入数据的时候，主键类型为字符串，ID为uuid的形式，插入时会导致分裂吗？**

会，特别不建议uuid做主键

---

**联合索引在B+树中的存储方式**

https://mengkang.net/1302.html

# 五、深入浅出索引二

在下面这个表 T 中，如果执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？

```mysql

create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;
insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241743747.png)

1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；
2. 再到 ID 索引树查到 ID=300 对应的 R3；
3. 在 k 索引树取下一个值 k=5，取得 ID=500；
4. 再回到 ID 索引树查到 ID=500 对应的 R4；
5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。

在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。

## 5.1.1 覆盖索引

> 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**

<!--覆盖索引就是在这次的查询中，所要的数据已经在这棵索引树的叶子结点上了-->

**在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？**

身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。

## 5.1.2 最左前缀原则

B+ 树索引结构，可以利用索引的“最左前缀”，来定位记录。

https://cloud.tencent.com/developer/news/44861

**对（name，age）这个联合索引进行分析**

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241812273.jpg)

索引项是按照索引定义里面出现的字段顺序排序的。

"where name like ‘张 %’" 走索引

单独的年龄不走索引

> 如果单独的字段不能走联合索引，那么考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引

只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

**在建立联合索引的时候，如何安排索引内的字段顺序。**

评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

## 5.1.3 索引下推

【索引下推】Index Condition Pushdown，简称 ICP。 是Mysql 5.6版本引入的技术优化。旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能。 ------------------ 如果查询利用到了索引下推ICP技术，在Explain输出的Extra字段中会有“Using index condition”。即代表本次查询会利用到索引，且会利用到索引下推。 ------------------ 索引下推技术的实现——在遍历索引的那一步，由只传入可以利用到的字段值，改成了多传入下推字段值。

---

不符合最左查询的字段的查询过程

```mysql
 select * from tuser where name like '张%' and age=10 and ismale=1;
```

这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。

在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241827519.jpg)

**总结：**

 1、覆盖索引：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据 

2、最左前缀：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符 

3、联合索引：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。

 4、索引下推：like 'hello%’and age >10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度

## FAQ

**关于日志处理**

分区表处理访问日志，drop分区

**优化器执行的顺序**

1.根据搜索条件找出所有可能用到的索引。
2.计算全表扫描的代价
3.计算其他索引扫描的代价
4.选出优化器计算出扫描成本最低的索引
（扫描代价涉及到I/O成本和CPU成本，innodb内部有一套自己的成本计算方法，简单举例比如：读取一个页到内存成本为1.0， 检测一条记录是否符合成本为0.2）

根据上面的逻辑，%j 这种条件使用了辅助索引。就是说，优化器认为“扫描辅助索引的代价比扫描聚集索引的代价要低”。

**为什么二级索引比主键索引小**

因为主键索引有额外的三个隐藏列，row_id，trx_id,roll_pointer,具体参考掘金小孩的mysql

**关于大数据表如何整理**

rename +新建表

**索引数据比数据大怎么处理**

删除数据的时候,当前删除索引被标记为被删除,改索引位置则可以被空间复用,当一页数据都被删除时页可以被复用,空间被复用但是磁盘系统表空间是未改变的,需要通过重建表,重建表的逻辑对数据紧凑排列来保存到临时文件中也就是online的逻辑对server端来说就是inplace逻辑

重建索引不行吗？

当没有开启`innodb_file_per_table`选项,导致所有的表(数据和索引)都存储在了一个文件中.
即使是使用了`optimize table`都无法释放空间.

可以参考这两篇文章:
https://stackoverflow.com/questions/1270944/mysql-innodb-not-releasing-disk-space-after-deleting-data-rows-from-table

https://stackoverflow.com/questions/3927690/howto-clean-a-mysql-innodb-storage-engine/4056261#4056261

**关于联合索引**

关于联合索引我的理解是这样的：比如一个联合索引(a,b,c)，其实质是按a,b,c的顺序拼接成了一个二进制字节数组，索引记录是按该字节数组逐字节比较排序的，所以其是先按a排序，再按b排序，再按c排序的，至于其为什么是按最左前缀匹配的也就显而易见了，没看过源码，不知道理解的对不对，希望老师指正。

给表创建索引时，应该创建哪些索引，每个索引应该包含哪些字段，字段的顺序怎么排列，这个问题没有标准答案，需要根据具体的业务来做权衡。不过有些思路还是可供参考的：
1.既然是一个权衡问题，没有办法保证所有的查询都高效，那就要优先保证高频的查询高效，较低频次的查询也尽可能的使用到尽可能长的最左前缀索引。可以借助pt-query-digest来采样统计业务查询语句的访问频度，可能需要迭代几次才能确定联合索引的最终字段及其排序。
2.业务是在演进的，所以索引也是要随着业务演进的，并不是索引建好了就万事大吉了，业务发生变化时，我们需要重新审视当初建的索引是不是还依然高效，依然能满足业务需求。
3.业内流传的有一些mysql 军规，其实这些并不是真正的军规，只是典型场景下的最佳实践。真正的军规其实就一条：高效的效满足业务需求。比如有个军规规定一个表上的索引数不超过5个，但如果我们现在有一些历史数据表、历史日志表，我们很明确的知道这些表上不会再有数据写入了，但我们的查询需求很多也很多样化，那我们在这些表上的索引数能不能超过5个？当然是没有任何问题的。当然关于这份军规还是要认真看一下的，但看的重点不是去记住它，而是要弄明白每一条军规它为什么这么规定，它这样规定是基于什么考虑，适用的场景和前提是什么，这些都弄明白了，你记不记得住这些军规都无所谓了，因为你已经把它溶化到了你的血液中，具体到自己的具体业务时游刃有余将是必然。

**公司有订单表，有些核心字段，比如订单号.时间(整型，时间戳，范围查找).订单状态（整型，6个值，可能in，可能=）.客户标识（整型，几百个值）.付款方式（整型，5个值），设备号（字符串，有权限需要in）,这6个字段后台都会用到查询筛选，而且不选的情况下条件就不传，按照联合索引最左原则，那么可能要建几十个索引，这是不可能的，这个表做了按月分表，数据量一张表大约1000万,不建立索引的话，后台选的条件没有建索引就会非常慢，强制最多只能查连续两个月的数据（union all），请有什么好的解决方案么？？？ **

得按照查询的模式，选最常见的来创建组合索引。比如如果时间+客户标识用得最多，就创建者两个的联合索引。

对于比较少用的条件，单独给这个字段建索引，然后查id出来跟别的字段的查询结果，在客户端取交集，也是一种思路。

**MySQL在执行一条SQL时，是如何选择使用哪个索引的。 possible keys有很多，根据什么选择用哪一个。**

索引统计信息、临时表成本、排序成本

**在不影响排序结果的情况下，在取出主键后，回表之前，会在对所有获取到的主键排序，请问是否存在这种情况？**

存在，在MySQL 里面叫做“MRR优化”

MRR（Multi Range Read) 是针对辅助索引的范围查询时， 会一次性读取多个主键的值，并进行排序后， 再一次性到主键索引进行回表。 这样可以将多次回表的随机IO转换成顺序IO，提升查询性能。

# 六、全局锁和表锁

MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作DML的时候，加 MDL 读锁；当要对表做结构变更操作DDL的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 如果对线上一个频繁DML操作的表做DDL如添加字段等操作，可能会导致死锁，使数据库连接资源被消耗完，导致数据库宕机。安全的解决方式是对表做DDL如添加字段时，设置执行语句的超时时间，写锁超时自动释放，不影响读锁。

MySQL 里面的锁大致可以分成

* 全局锁
* 表级锁
* 行锁

## 6.1 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是` Flush tables with read lock (FTWRL)`。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。<!--解锁unlock tables 可以给从库加读锁-->

**全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。**

---

**mysqldump 使用参数–single-transaction进行备份可以开启事务为什么需要FTWRL？**

致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。

single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。

**为什么不使用 set global readonly=true 而使用FTWRL？**

set global readonly=true   方式也可以让全库进入只读状态

一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。

二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

三是、在 slave 上 如果用户有超级权限的话 readonly 是失效的

业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，对里面任何一个表做加字段操作，都是会被锁住的。

## 6.2 表级锁

MySQL 里面表级别的锁有两种：一种是表锁（lock tables 表名 read write / unlock tables），一种是元数据锁（meta data lock，MDL)。

> 与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

### 6.2.1 lock tables 

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。

### 6.2.2 MDL

另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

> 元数据锁是server层的锁，表级锁，主要用于隔离DML（Data Manipulation Language，数据操纵语言，如select）和DDL（Data Definition Language，数据定义语言，如改表头新增一列）操作之间的干扰。每执行一条DML、DDL语句时都会申请MDL锁，DML操作需要MDL读锁，DDL操作需要MDL写锁（MDL加锁过程是系统自动控制，无法直接干预，读读共享，读写互斥，写写互斥）

因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。

* 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
* 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

### 6.2.3 为什么给一个小表加个字段，导致整个库挂了

> 给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表 t 是一个小表。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111251636592.jpg)

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。

之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。

如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

参考

https://blog.csdn.net/q2878948/article/details/96430129

### 6.2.4 如何安全地给小表加字段？

1. 解决长事务，事务不提交，就会一直占着 MDL 锁。要考虑先暂停 DDL，或者 kill 掉这个长事务。

2. 比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。

```mysql
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ... 
```

## 总结

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：

1. 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
2. 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。
3. 

MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## FAQ

**备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？**

**当备库用–single-transaction 做逻辑备份的时候，如果从主库的 binlog 传来一个 DDL 语句会怎么样？**

假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：

```mysql
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
/***开启一致性视图，一致性试图不包含表结构**/
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */
```

在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);

启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；

设置一个保存点，这个很重要（Q3）；

> 两阶段锁，事务回滚或者提交时，才会释放锁。Q6之后还需要备份其他表。备份期间会占用MDL读锁，设置回滚点，读完数据后，回滚释放锁。将锁的占用时间控制到最短。
>
> 备份过程中会有多个mdl锁，一个表一个（假设有tn个表）如果不设置savepoint，第一个表读完后，表1的dml锁还不会释放，会一直等tn的表读完，事务提交才会释放所有表的锁，所以时间会很长

show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。<!--ROLLBACK TO SAVEPOINT：目的是释放锁-->

DDL 从主库传过来的时间按照效果不同，下面有四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成

1. 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。
2. 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止；<!--获取表结构和后面的select是强相关的，但是到这时还没有加锁，因此，此时是可以执行dll语句的，当获取表结构后再select的时候发现表结构变更了就会报错，估计是为了备份创建的表结构和当前的结构不匹配导致的-->
3. 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。<!--已经持有了 MDL 的读锁，阻塞 binlog 的 DDL 操作。-->
4. 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。<!--从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。-->

------

**online ddl**

Online DDL的过程是这样的：
1. 拿MDL写锁
2. 降级成MDL读锁
3. 真正做DDL
4. 升级成MDL写锁
5. 释放MDL锁

**全局锁和表锁是Server层实现的**

**如何对大表添加字段**

使用Gh-ost

**关于表字段设计的建议**

建议全小写和下划线



# 七、行锁，如何减少行锁对性能的影响

> MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

## 7.1 两阶段锁

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

**如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：

1. 从顾客 A 账户余额中扣除电影票价；
2. 给影院 B 的账户余额增加这张电影票价；
3. 记录一条交易日志。

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

<!--需要进行死锁检测，即使只有100个线程事务，但死锁检测的复杂度是o(n^2)，会需要10000个数量级的检测，所以出现cpu消耗高，并发事务却没多少的情况-->

如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？

## 7.2 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261024389.jpg)

这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

* 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。

* 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数` innodb_deadlock_detect `设置为 on，表示开启这个逻辑。

在 InnoDB 中，`innodb_lock_wait_timeout `的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。<!--死锁的处理策略，破坏死锁的几个条件 互斥 占有并等待 不可剥夺 循环等待-->

你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

<!--过程示例：新来的线程F，被锁了后就要检查锁住F的线程（假设为D）是否被锁，如果没有被锁，则没有死锁，如果被锁了，还要查看锁住线程D的是谁，如果是F，那么肯定死锁了，如果不是F（假设为B），那么就要继续判断锁住线程B的是谁，一直走知道发现线程没有被锁（无死锁）或者被F锁住（死锁）才会终止-->

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

### 7.2.1 如何解决由这种热点行更新导致的性能问题呢？

* **可以将临时思死锁检测关闭** 但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
* **控制并发度** 比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。

* **对更新同一行的请求进行排队**，对于相同行的更新，在进入引擎前排队，这样在 InnoDB 内部就不会有大量的死锁检测工作了。
* **设计角度，将一行数据变成多行** 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。相当于子账户的概念，原理上就是分段汇总，Java原子类LongAdder也使用了这个原理。这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。

## 结论

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。

减少死锁的主要方向，就是控制访问相同资源的并发事务量。

## FAQ

**如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：**

* 第一种，直接执行 delete from T limit 10000;
* 第二种，在一个连接中循环执行 20 次 delete from T limit 500 order by id;
* 第三种，在 20 个连接中同时执行 delete from T limit 500。



你会选择哪一种方法呢？为什么呢？

如果一定要在这三个中选，肯定选第二个。 第一个事务太长，执行时间过长， 如果是主备形式的，影响数据同步的时间。 这么多数据如果回滚的话，那该是多痛苦的事情 加锁的时间过长，会造成锁超时的 第三个，很明显有并发问题，如果产生循环等待就是死锁了。 其实可以把id利用起来，20个连接，每个都删500个id，岂不更好。

第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。<!--从库回放relay log的时候也会锁很久（占用MDL锁，导致同步DDL的binlog延迟），导致主库同步过来的binlog阻塞，造成主从延迟-->

第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。

**关于死锁检测innodb_deadlock_detect，每条事务执行前都会进行检测吗？**

如果他要加锁访问的行上有锁，他才要检测。

1. 一致性读不会加锁，就不需要做死锁检测；

2. 并不是每次死锁检测都都要扫所有事务。比如某个时刻，事务等待状态是这样的：

  B在等A，
  D在等C，
  现在来了一个E，发现E需要等D，那么E就判断跟D、C是否会形成死锁，这个检测不用管B和A

---

**innodb行级锁是通过锁索引记录实现的。如果update的列没建索引，即使只update一条记录也会锁定整张表吗？**

如果列上没有索引，更新就是走主键索引树，逐行扫描满足条件的行，等于将主键索引所有的行上了锁，假如加上limit 1,扫描 主键索引树，直到找到第一条满足条件的行，扫描过的行都会被加上行锁。即为->如果update的列没建索引，即使只update一条记录也会锁定整张表

1. 当加上limit1之后 更新语句的执行流程是先去查询在去更新,也就是查询sql为 select * from t where name = "abc" limit 1 for update,相当于扫描主键索引找到第一个满足name="abc"的条件为止,此时锁的区间为(负无穷,当前行的id],如果在这个id之后的更新和插入时都不会锁住的,在这个id之前的更新和插入会阻塞,之后则不会阻塞

2. 如果不加limit 1的话,因为此时是整个主键索引全表扫描则整个表锁住了

3. 回表的行锁,比如字段name有普通索引,在更新操作时普通索引会锁住的同时,如果更新操作需要回表的话对应的主键索引也会存在锁(主键索引锁临界锁会退化为行锁),普通索引(间隙锁和行锁)

 如果隔离级别是rr的话，那就说明你不能出现不可重复读，所以当你在用没有索引的字段当where条件更新的话，并且没有加limit，那么mysql会扫描主键然后锁住所有的行，因为他就是要保证他在更新的时候，别的事务不能搞事情啊，要别人插入一条，或者更新了一条，恰好又满足我刚才那个where条件 ，那就违反了当前的隔离级别了，当然rc条件下，就无所谓了，反正我的隔离级别下会出现不可重复读，别人要要更新那请便，只要不搞我当前扫描到这行就行了（因为他加了行锁）。加了limit 亦然，rr下，我扫描到的这前多少行你不能给我搞事情，搞了就违反我的隔离级别，所以他锁住了前多少行，后面爱咋搞咋搞。rc也无所谓，也只是行锁，锁住那几行。

---

**不支持行锁的引擎，只能使用表锁，而表锁同一张表在同一时刻只能有一个更新。但是上节课讲的表级锁中的MDL锁，dml语句会产生MDL读锁，而MDL读锁不是互斥的，也就是说一张表可以同时有多个dml语句操作。感觉这两种说法有点矛盾**

不矛盾，MDL锁和表锁是两个不同的结构。

比如：
你要在myisam 表上更新一行，那么会加MDL读锁和表的写锁；
然后同时另外一个线程要更新这个表上另外一行，也要加MDL读锁和表写锁。

第二个线程的*MDL读锁是能成功加上*的，但是被表写锁堵住了。从语句现象上看，就是第二个线程要等第一个线程执行完成。

**update会持有读MDL。读和读不互斥。但是对于行锁来说。两个update同时更新一条数据是互斥的。这个是因为多种锁同时存在时，以粒度最小的锁为准的原因么？**

不是“以粒度最小为准”
而是如果有多种锁，必须得“`全部不互斥`”才能并行，只要有一个互斥，就得等。

**如果开启事务，然后进行死锁检测，如果发现有其它线程因为这个线程的加入，导致其它线程的死锁，这个流程能帮着分析一下么**

理论上说，之前没死锁，现在A加进来，出现了死锁，那么死锁的环里面肯定包含A，
因此只要从A出发去扫就好了

**如何在死锁发生时,就把发生的sql语句抓出来,？**

 show engine innodb status 里面有信息，不过不是很全…

> 一般中间件每次执行事务时，都会重置状态。比方说spring托管的事务会有重置代码，最明显的就是在spring事务切面的代码里，从getConnection()时就有类似的代码。对于发生死锁如何排查，一般是dba去做，其实mysql库表里是有的，开发一般是不给权限查的，这里只贴下自己写的，做抛砖引玉
> SELECT r.trx_id waiting_trx_id,
>    r.trx_mysql_thread_id waiting_thread,
>    r.trx_query waiting_query,
>    b.trx_id blocking_trx_id,
>    b.trx_mysql_thread_id blocking_thread,
>    b.trx_query blocking_query
> FROM information_schema.innodb_lock_waits w
>    INNER JOIN information_schema.innodb_trx b
>        ON b.trx_id = w.blocking_trx_id
>    INNER JOIN information_schema.innodb_trx r
>        ON r.trx_id = w.requesting_trx_id;

**在使用连接池的情况下,连接会复用.比如一个业务使用连接set sql_select_limit=1,释放掉以后.其他业务复用该连接时,这个参数也生效.请问怎么避免这种情况,或者怎么禁止业务set session？**

 5.7的reset_connection接口可以考虑一下

> 在业务里，比如使用mybatis使用数据库的连接池，一个事务获取一个连接时，另一个事务这时是获取不到这个连接的，只有一个事务执行完释放连接到连接池，其它事务才能获取到，而释放后的连接已经被恢复到获取时的状态，包括自动提交等设置

**双11的成交额,是通过redis累加的嘛？**

用redis的话，为了避免超卖需要增加了很多机制来保证。修改都在数据库里执行就方便点。前提是要解决热点问题

**关于使用char类型id索引失效问题**

原因是id 是char 类型，但是没有加单引号，所以没有进入id索引中，然后锁表了，所以导致死锁。

sql：delete from 表A where id =15426169754750004759008 STORAGEDB
(id是主键)

会出现隐式转换函数, ID索引会失效, 走全表扫描



# 八、事务隔离

```mysql
CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB;insert into t(id, k) values(1,1),(2,2);
```

![事务 A、B、C 的执行流程](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261434878.png)

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。<!--数据库在设计的时候，一定会考虑最大程度的支持事务之间的并发，那它一定会让锁的时间尽可能短-->

第一种启动方式，一致性视图是在执行第一个==快照读语句==（select ）时创建的；

第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。

在 MySQL 里，有两个“视图”的概念：

> 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。
>
> 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。
>
> 它没有物理结构，通过高低水位，数据版本号，undo日记来进行判断数据可见不可见，作用是事务执行期间用来定义“我能看到什么数据”。

## 8.1 “快照”在 MVCC 里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。<!--基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)
如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错(参考第六章)-->

InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261456872.png)

要获取旧版本的数据行时，可以通过最新版本的数据和最新版本到目的版本之间的 Undo Logs 计算出来，因为 Undo Logs 记录了每个对应版本对应行数据的值。 Undo Logs 中分为两种类型: 1. INSERT_UNDO（INSERT操作），记录插入的唯一键值； 2. UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。

事务执行之后，其他事务的更新对它虽然不可见，但是数据版本还是可见的，因为数据库实际上存储的是最新版本的数据。但是对于该事务来说，需要根据版本号以及Undo Logs计算出他需要的版本对应的数据

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

## 8.2 视图

**在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。**

<!--判断事物开启条件的一种是begin 并且执行第一个'操作 'InnoDB 表的语句，所以我第一句只执行非select的dml语句就会生成事务id，但是并不会生成一致性视图，在我要执行select的时候 其它事物可能已经创建。所以对于视图数组就会出现比当前事务id还大并且没有提交的事物，同理也会出现比当前事务还小且没有提交的事物。所以假如当前事务id为88 ，活跃数组就可能有[72,79,88,90,91] 不连续的原因也知道吧，就是因为在生成一致性视图的时候中间短事务早就提交了-->

事务id再当前读才会申请而UPDATE、DELETE、INSERT、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE是当前读。

数组里面事务 ID 的最小值记为低水位，==当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位==。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

**为什么数组中要保存当前所有事务ID**

> Innodb 要保证这个规则：事务启动以前所有还没提交的事务，它都不可见。
>
> 但是只存一个已经提交事务的最大值是不够的。 因为存在一个问题，那些比最大值小的事务，之后也可能更新（就是你说的98这个事务）
>
> 所以事务启动的时候还要保存“现在正在执行的所有事物ID列表”，如果一个row trx_id在这列表中，也要不可见。

而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261518663.png)

<!--这张图应该是在申请事务ID时，事务的状态-->

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

比如，对于图 2 中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。

**InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。



**反向推到为什么不直接判断是否在视图数组中**

> 为什么不直接判断该事务ID是否存在于活跃事务数组中呢？这样还简便得多呀！
>    确实，这样实现起来简单，但效率不行。上述方式，虽然实现和理解起来稍微复杂，但效率很高，实现它的一个前提条件就是事务ID要根据创建时间有序递增，才能快速鉴别出一定可见和一定不可见的情况（情况1、2）。其实我觉得实现的原理思想和布隆过滤器的思想很相似，先快速鉴别出一定可见和不可见的数据（第1、2种），然后对于可能可见和可能不可见的数据（第3种）通过活跃数组进行精确判断。

**当开启事务时，需要保存活跃事务的数组（A），然后获取高水位（B）。在这两个动作之间（A和B之间）会不会产生新的事务？如果产生了新的事务，那么这个新的事务相对于当前事务就是可见的，不管有没有提交?**

> 代码实现上，获取视图数组和高水位是在事务系统的锁保护下做的，可以认为是原子操作，期间不能创建事务。

## 8.3 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**

**除了 update 语句外，select 语句如果加锁，也是当前读。**

下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。

```mysql

mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

## 8.4 小结

**InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。**

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

而当前读，总是读取已经提交完成的最新版本。

**为什么表结构不支持“可重复读”？**

这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。

**8.0已经把表结构放到InnoDB字典里，表结构支持可重复读。在mysqldump过程中修改表结构并不会导致程序终止了。**

## FAQ

### 减库存场景

当前库存：num=200
假如多线程并发：
AB同时开启事务，A先请求到行锁，

| 事务A                                                   | 事务B                                                   |
| ------------------------------------------------------- | ------------------------------------------------------- |
| start transaction;                                      | start transaction;                                      |
| select num from t where num>0;先查询当前库存值（num>0） |                                                         |
| update t set num=num-200; 库存减量                      | select num from t where num>0;先查询当前库存值（num>0） |
| Commit                                                  | update t set num=num-200; 库存减量                      |
|                                                         | Commit                                                  |


A：查询到num=200,做了库存减量成了0
B：事务启动后，查询到也是200，等 A 释放了行锁，B进行update，直接变成 -200
但是 B 查询时，时有库存的，因此才减库存，结果变成负的。

给 select 加读锁或者写锁吗 ？这种select 加锁，对业务影响大吗？

> 一开始Select 加锁虽然可以，但是会比较严重地影响并发数。
>
> 比较简单的做法是update语句的where 部分加一个条件： where nun >=200 .
> 然后在程序里判断这个update 语句的affected_rows,
> 如果等于1 那就是符合预期；
> 如果等于0，那表示库存不够减了，业务要处理一下去，比如提示“库存不足”

**思考**

如果使用乐观锁的话但在并发条件下更新，完全的cas会导致较多的更新失败。其实这里解决的是num不能被减成负，所以使用num>=200来保证就可以了。既保证业务正确，也提高并发性能，解决并发量适中的场景也只能这样. 如果并发太高, 那就是hot key problem, 后来的写入会timeout. 这就要用(写)缓存或者消息队列来辅助处理了.

# 九、普通索引与唯一索引如何选择

## 9.1 索引

### 9.1.1 概念

索引就好比一本书的目录，它会让你更快的找到内容，显然目录(索引)并不是越多越好，假如这本书1000页，有500也是目录，它当然效率低，目录是要占纸张的,而索引是要占磁盘空间的。

### 9.1.2 Mysql索引结构

Mysql索引主要有两种结构：B+树和hash.

* **hash:**hsah索引在mysql比较少用,他以把数据的索引以hash形式组织起来,因此当查找某一条记录的时候,速度非常快.当时因为是hash结构,每个键只对应一个值,而且是散列的方式分布.所以他并不支持范围查找和排序等功能.

* **B+树:**b+tree是mysql使用最频繁的一个索引数据结构,数据结构以平衡树的形式来组织,因为是树型结构,所以更适合用来处理排序,范围查找等功能.相对hash索引,B+树在查找单条记录的速度虽然比不上hash索引,但是因为更适合排序等操作,所以他更受用户的欢迎.毕竟不可能只对数据库进行单条记录的操作.

### 9.1.3 Mysql常见索引

* **主键索引(PRIMARY KEY()：**  是一种特殊的唯一索引，不允许有空值。

  ```mysql
  ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 
  ```

  

* **唯一索引(UNIQUE)：**与"普通索引"类似，不同的就是：索引列的值必须唯一，但允许有空值。

  ```mysql
   ALTER TABLE `table_name` ADD UNIQUE (`column`)
  ```

  

* **普通索引(INDEX)：**最基本的索引，没有任何限制

  ```mysql
    ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 
  ```

  

* **全文索引(FULLTEXT)：**仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时耗空间。

  ```mysql
   ALTER TABLE `table_name` ADD FULLTEXT ( `column` )
  ```

  

* **组合索引：**为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。

  ```mysql
  ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` )
  ```

  

## 9.2 性能分析





![InnoDB 的索引组织结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111271940806.png)

### 9.2.1 查询过程

假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。

* 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。
* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

**这种不同对查询的性能微乎其微**

InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。

因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。

当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。

但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。

### 9.2.2 更新过程

#### 9.2.2.1 change buffer

> insert buffer是对于非唯一辅助索引更新的优化。对于主键索引来说我们可以通过设置increment_id来实现顺序插入，但是对于辅助索引往往难以保证更新的顺序性，这样可能会导致每次对于辅助索引insert都是离散的，需要遍历整张表找到插入位置，从IO角度来说效率低，而且会将无用页读入内存占用空间，所以使用insert buffer来实现对于索引延时更新，当发生insert的时候，如果索引页在内存中，则更新，不再内存中则将更新记录到 insert buffer中，当索引页被读入到内存的时候再执行 merge 操作。 后面的innodb版本，不仅支持insert，同时还支持 update ，delete操作所以称为change buffer！！！

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。<!--这里说的数据页，指的是二级索引树的数据页，并不是聚簇索引即主键树的数据页。 如涉及到索引字段的更新，也是要更新对应的索引数据的，而索引树对应的数据页不在内存中，则changeBuffer会先保存这个数据，之后会和对应的数据页进行merge过程。 redoLog也是会记录这一动作的，所以更新对应索引树的数据不会丢失。-->

需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。<!--change buffer可以看成也是一个数据页，需要被持久化到 系统表空间（ibdata1），以及把这个change buffer页的改动记录在redo log里，事后刷进系统表空间（ibdata1）。-->

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

**为什么要使用change buffer ？**

首先要明确概念： 

1. mysql数据存储在主键索引树的叶子节点。 

2. 普通索引和唯一索引也都有自己的索引树，树的叶子节点存储的是主键ID。

3. 做更新操作（插入，更新，删除）会同时更新所有的索引树结构。---------insert：主键索引树和唯一建索引树的肯定都要更新，肯定是无法用到change buffer的；但是普通索引树的更新，是可以使用change buffer的。 update：只要涉及到相关字段更新，就要同时更新相应的索引树。道理同上。 【显然，insert操作的影响更大，如果有多个唯一索引，insert对内存命中率会有极大影响】 


1. 减少读磁盘：仅仅是减少的是对二级普通索引页的读磁盘操作，而对于其他类型的页(唯一索引，主键索引)还是要读磁盘的。 
2. 减少内存占用：change buffer虽然还是需要内存占用(记录数据更新动作)，但相比于数据页来说(默认16K)，所占的内存还是小了很多的。



**什么时候使用change buffer，以及为什么唯一索引不使用**

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。

唯一索引更新导致性能降低的原因应该是：必须把数据页加载到内存中进行判断涉及到的随机io的读写

change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 `innodb_change_buffer_max_size`来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。

**如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。**

第一种情况是，**这个记录要更新的目标页在内存中**。这时，InnoDB 的处理流程如下：

* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。

这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。

第二种情况是，**这个记录要更新的目标页不在内存中**。这时，InnoDB 的处理流程如下：

* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

某个业务的库内存命中率突然从 99% 降低到了 75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引。<!--注意这里是insert操作。insert操作必然要更新所有的索引树，当然就包括了唯一索引树，那么在更新这棵树的时候就用不上change buffer，就需要频繁访问磁盘，降低内存命中率。而update操作，只要不涉及到唯一索引字段的更新，就不会去更新唯一索引树，所以update操作的消耗是比较低的。所以涉及到有大量insert的业务场景，发生内存命中率低的问题，就需要考虑是不是索引的问题了。-->

## 9.3 change buffer 的使用场景

> change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。那么.

**现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？**

因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。

因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。

## 9.4 索引选择和实践

这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。

特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。<!--也就是读少写多的场景-->

## 9.5 什么是buffer pool

> 硬盘在读写速度上相比内存有着数量级差距，如果每次读写都要从磁盘加载相应数据页，DB的效率就上不来，因而为了化解这个困局，几乎所有的DB都会把缓存池当做标配（在内存中开辟的一整块空间，由引擎利用一些命中算法和淘汰算法负责维护和管理），change buffer则更进一步，把在内存中更新就能可以立即返回执行结果并且满足一致性约束（显式或隐式定义的约束条件）的记录也暂时放在缓存池中，这样大大减少了磁盘IO操作的几率

## 9.6 change buffer 和 redo log

理解了 change buffer 的原理，你可能会联想到我在前面文章中和你介绍过的 redo log 和 WAL。

> redo log 与 change buffer(含磁盘持久化) 这2个机制，不同之处在于优化了整个变更流程的不同阶段。 先不考虑redo log、change buffer机制，简化抽象一个变更(insert、update、delete)流程： 
>
> 1、从磁盘读取待变更的行所在的数据页，读取至内存页中。
>
>  2、对内存页中的行，执行变更操作 
>
> 3、将变更后的数据页，写入至磁盘中。
>
>  步骤1，涉及 随机 读磁盘IO； 步骤3，涉及 随机 写磁盘IO；
>
>  --change buffer机制，优化了步骤1,避免了随机读磁盘IO; 
>
> --redo log机制， 优化了步骤3,避免了随机写磁盘IO，将随机写磁盘，优化为了顺序写磁盘(写redo log，确保crash-safe) ；
>
>  --在我们mysql innodb中， change buffer机制不是一直会被应用到，仅当待操作的数据页当前不在内存中，需要先读磁盘加载数据页时，change buffer才有用武之地。 redo log机制，为了保证crash-safe，一直都会用到。

**对于以下语句分析执行流程**

```mysql
insert into t(id,k) values(id1,k1),(id2,k2);
```

这里，我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图。

![change buffer 的更新过程](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111281556442.png)

* 内存、
* redo log（ib_log_fileX）、
*  数据表空间（t.ibd）、数据表空间：就是一个个的表数据文件，对应的磁盘文件就是“表名.ibd”；
* 系统表空间（ibdata1）。系统表空间：用来放系统信息，如数据字典等，对应的磁盘文件是“ibdata1”

这条更新语句做了如下的操作（按照图中的数字顺序）：

1. Page 1 在内存中，直接更新内存；

2. Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”

3. 这个信息将上述两个动作记入 redo log 中（图中 3 和 4）。

做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。

**那么在这之后新来了一个select请求如何处理**

比如，我们现在要执行 select * from t where k in (k1, k2)。这里，我画了这两个读请求的流程图。

如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，我在图中就没画出这两部分。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111281600997.png)

从图中可以看到：

1. 读 Page 1 的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。
2. 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。<!--merge的时机应该是读取数据页到内存中的时候，这时会先在内存的change buffer找有没有修改，没找到继续到磁盘的change buffer找，如果都没有找到证明这个数据页没有修改。
   change buffer内容会通过redo log刷到磁盘页吗？不会，是通过内存/磁盘中的change buffer刷的。-->

所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**

## 小结

redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成；redo日志记录的是change buffer页的变化。 change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd

唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。

1. changebuffer跟普通数据页一样也是存在磁盘里，区别在于changebuffer是在共享表空间ibdata1里
2. redolog有两种，一种记录普通数据页的改动，一种记录changebuffer的改动
3. 只要内存里脏页（innodb buffer pool）里的数据发生了变化，就一定会记录2中前一种redolog
   （对数据的修改记录在changebuffer里的时候，内存里是没有这个物理页的，不存在脏页）
4. 真正对磁盘数据页的修改是通过将内存里脏页的数据刷回磁盘来完成的，而不是根据redolog



change Buffer和数据页一样，也是物理页的一个组成部分，数据结构也是一颗B+树，这棵B+树放在共享表空间中，默认ibdata1中。change buffer 写入系统表空间机制应该和普通表的脏页刷新到磁盘是相同的机制--Checkpoint机制；之所以change buffer要写入系统表空间，是为了保证数据的一致性，change buffer做修改时需要写redo log，在做恢复时需要根据redo log来恢复change buffer，若是不进行change buffer写入系统表空间，也就是不进行持久化，那么在change buffer写入内存后掉电（也就是篇尾提出的问题），则无法进行数据恢复。这样也会导致索引中的数据和相应表的相应列中的数据不一致。change buffer 写入到了系统表空间，merge 的时候会先查询change buffer里对应的记录，然后进行purge，因为change buffer B+树的key是表空间ID，所以查询根据表空间ID 查询change buffer会很快。



**注意**

> redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成；redo日志记录的是change buffer页的变化。
> change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd。

首先明确一个观点，redo log最大的作用，就是用于数据库异常宕机的恢复工作。
如果数据库永远不会宕机，那么不需要 redo log。redo log 和 change buffer其实关注的是两个事情，不能混为一谈。

其次，数据库缓冲池中有如下内容：数据页，索引页，插入缓冲，等等其他页（其他页目前不需要了解），数据页可以理解为叶子节点，索引页可以理解为非叶子节点，插入缓冲就是老师这节课讲的 change buffer。

当做insert update delete操作时，会涉及到两方面的更新，一类是主键索引B+树，另一类的非主键索引B+树。针对，主键索引B+树和 非主键索引中的唯一索引B+树，如果在内存中，直接更新内存；如果不在内存，直接从数据库中读取页到内存中来，更新内存即可。针对非主键索引的普通索引B+树，如果树在内存中，直接更新内存；如果不在内存中，更新change buffer，等到后面需要使用这个树的时候，会从磁盘中读取，然后做merge操作。

有同学问，到底是依据change buffer磁盘还是依据redo log更新磁盘，我的回答是，他们都不会直接更新磁盘，刷新磁盘的工作是innodb存储引擎中的线程去做的。redo log负责的是 异常宕机的恢复；change buffer用于 提高普通索引更新的性能。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111282128560.png)

## FAQ

**你可以看到，change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？**



---

**如果是针对非唯一索引和唯一索引的更新和delete而且条件是where 索引值=这种情况,是否二级索引和唯一索引就没有区别呢**

这时候要“先读后写”，读的时候数据会读入内存，更新的时候直接改内存，就不需要change buffer了

change buffer就是为了延迟更新数据的时候对二级索引的更新，而where 索引值=，就是用的二级索引来更新的，更新之前得先把二级索引的树读出来，既然已经读出来了，就可以直接更新了，没必要用change buffer了

**如何判断内存命中率**

Hit rate

**change buffer相当于推迟了更新操作，那对并发控制相关的是否有影响，比如加锁？我一直以为加锁需要把具体的数据页读到内存中来，才能加锁，然而并不是？**

锁是一个单独的数据结构，如果数据页上有锁，change buffer 在判断“是否能用”的时候，就会认为否

change Buffer 针对的对象是索引页，锁是加在数据页上的吧。如果更新的时候发现数据页已经有锁了，change Buffer自然会失效 

**Merge 行为之后还会产生redo log吗？**

第一步，merge其实是从磁盘读数据页到内存，然后应用，这一步都是更新的内存，同时写redolog

现在内存变成脏页了，跟磁盘数据不一样。之后就走刷脏页的流程。刷脏页也不用写。

个人理解会产生的因为第一次redo log 记录的是change buffer对应的页，而merge过程记录的是内存页

**为什么change buffer已经使用了redo log 还需要持久化**

猜想大概是因为change buffer 需要快速的检束出数据以便于进行merge操作，所以必须要保持B+树这种高效的结构，再者就是redo log 的空间有限，不能存很多数据

当数据库崩溃时可以通过redo log将change buffer内容回放出来。” 是的，所以change buffer其实也是用了WAL机制。

**change buffer中存的内容是“在某个数据页更新什么”，但是在update/insert时，确定这条记录更新/插入在哪个数据页，不也是有一个查找的过程么？（肯定有一个一层层查找的过程，会路过很多数据页啊）为了确定在哪个数据页操作而遍历过的数据页也会读进内存作缓存吗？**

是的，查找过程避不开，但是二级索引树的非叶子节点没多少，主要在磁盘上的还是叶子节点。

准确来说在二级索引的叶子节点的上一层节点中就可以确定要插入到哪一页中，因为在二级索引的叶子节点时索引列+主键值，他的非叶子节点中每页存储的是叶子节点中每一页中最小的那个索引列+主键的一条记录+页号，当我们插入的时候就可以从最后一层的非叶子节点中找到应该插入的页

查非叶子节点，都是在内存中进行的。查叶子节点，并且将这页载入到内存， 这个比较耗时，特别是在写多读少的场景下，性能差很多

**如果说因为内存不足需要回收change buffer这部分内存，那也应当将数据merge后刷入磁盘吧。**

“内存不足需要回收change buffer这部分内存“，只需要让change buffer本身持久化可以，不需要执行merge操作。merge操作是在读数据页的时候做的

**update操作不是先读后写吗？如果是先读的话，不是应该把数据已经读到内存了吗？那这样的话直接更新内存不就好了，为什么还要写change buffer**

 如果一个表上有字段 a, b, 且有普通索引c， update语句为 update xxx set a = 'xxx' where c = 'xxx'， 执行这个语句的时候， 因为要判断c = 'xxx' 的记录是否存在， 存在的话才会更新， 此时必定要将c 索引的内存页载入到内存中（执行计划会走索引c) ， 修改内存页，写redo log; 同时主键索引页也要载入内存进行修改（主键索引永远都需要先读后写，这个免不了）。这个就用不到Change Buffer。
但是如果是另外一种场景： update xxx set a = 'xxx' where b = 'xxx' ， 这个时候因为查询条件不走索引c，故不需要将c的数据页载入内存， 针对索引c上的修改就写入到Change Buffer， 但是针对主键索引，还是要先载入内存再修改。 此时的优化就是针对索引c的页的随机io的优化。

**关于change buffer 主要节省的则是随机读磁盘的 IO消耗这个点，我的理解是如果没有change buffer机制，那么在执行更新后（写入redolog），读取数据的时候需要从次磁盘随机读取redolog合并到数据中，主要减少的是这部分消耗？**

不是，如果没有change buffer, 执行更新的“当时那一刻”，就要求从磁盘把数据页读出来（这个操作是随机读）。

**数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。后面又说change是占用pool内存的，那到底占不占用buffer pool的内存**

1. change buffer本身是占用内存的；
2. 但是chage buffer本身只是记录了“更新过程”，远远比数据页（一个16k）小。相比于把数据页读入内存，这个方式还是省了内存的。

**change buffer的持久化是不是考虑到随着事务的运行，内存中已经存放不下change buffer了，所以才考虑要持久化到系统表空间中去的？**

change buffer的写盘策略跟数据一样，内存放不下会触发落盘，还有checkpoint推进的时候也是可能会出发



**对于二级普通索引，update t where k=？这种语句，执行的时候，是先读后写吗，也就是先按k索引树将相应索引页都加载到内存，然后再按主键索引将数据页面加载到内存？还是按照文中提到的先记录到change buffer？**

如果是where k= N 是要先读的（确保数据存在）

读的是主键b+树，带数据，但是不用读普通索引的B+树，意思就是change buffer只缓存了普通索引的B+树的操作，只有要使用这个索引的时候，这些操作才能真正生效，

**changeBuffer 更新过程**

changeBuffer详细一点来说是对普通索引页的优化。
普通索引（name） （age）主键（id）
（4，张三）
语句一:update name=李四 where id = 4
语句二:update age where name='李四'
可知主键为聚簇索引，存着整行数据。普通索引存着当前索引段的数据和主键值。
语句一:
1.主键数据（页）读到内存，修改主键上的数据行。
2.修改普通索引name上的name字段数据。
第一步是必须读盘到内存中的，然后修改对应数据,得到修改数据行。第二步是要去修改name索引的数据，这时就可以将此操作放到changeBuffer里，等待下次用到name索引时（用到就会读到内存中），再将其修改，所以就省了第二步的读盘操作。所以普通索引的name暂时是张三。
语句二（紧接着语句一）:
1.name索引数据读到内存，根据hash发现changBuffer上有本次读取页的数据没有改，就顺便改一下。
2.索引到name=李四的id,然后去修改主键中的age数据，修改完主键的后，这时就该去修改age索引的数据了，但age是普通索引，所以会将操作放到changeBuffer里，等待时机去更新。

**在不读磁盘的情况下，delete/update影响行数如何获取？**

不可能完全不读盘的，只是说可能某些二级索引可以不用读盘，不管做的是insert, update还是delete操作，都涉及到主键索引上数据的更新，主键索引的相关页都是需要载入内存的，通过主键索引的变化就可以获取影响行数

# 十、MySQL为什么有时候会选错索引

首先创建一个表

```mysql
CREATE TABLE `t` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `a` int(11) DEFAULT NULL,
  `b` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `a` (`a`),
  KEY `b` (`b`)
) ENGINE=InnoDB;
```

然后，我们在表 t 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)。

```mysql
create procedure idata()
begin
declare i int;
set i=1;
start TRANSACTION;
while(i<=100000)do
insert into t(a, b) values(i, i);
set i=i+1;
end while;
commit;
end;


```

接下来，我们分析一条 SQL 语句：

```mysql
select * from t where a between 10000 and 20000;
```

![image-20211207111653178](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112071116283.png)

使用以下的方法进行测试

![session A 和 session B 的执行流程](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112071117791.png)

随后，session B 把数据都删除后，又调用了 idata 这个存储过程，插入了 10 万行数据。这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了。我们可以通过慢查询日志（slow log）来查看一下具体的执行情况。

为了说明优化器选择的结果是否正确，我增加了一个对照，即：使用 `force index(a) `来让优化器强制使用索引 a

下面的三条 SQL 语句，就是这个实验过程。

```mysql
set long_query_time=0;
select * from t where a between 10000 and 20000; /*Q1*/
select * from t force index(a) where a between 10000 and 20000;/*Q2*/
```

* 第一句，是将慢查询日志的阈值设置为 0，表示这个线程接下来的语句都会被记录入慢查询日志中；
* 第二句，Q1 是 session B 原来的查询；
*  第三句，Q2 是加了 force index(a) 来和 session B 原来的查询语句执行情况对比。

下面是慢查询日志

```mysql
# Time: 2021-12-07T02:51:36.018162Z
# User@Host: root[root] @  [172.18.0.1]  Id:  1545
# Query_time: 0.034452  Lock_time: 0.000118 Rows_sent: 10001  Rows_examined: 100000
SET timestamp=1638845495;
/* ApplicationName=DataGrip 2021.3.1 */ select * from t where a between 10000 and 20000;
```

```mysql
# Time: 2021-12-07T02:52:35.324978Z
# User@Host: root[root] @  [172.18.0.1]  Id:  1545
# Query_time: 0.009915  Lock_time: 0.000091 Rows_sent: 10001  Rows_examined: 10001
SET timestamp=1638845555;
/* ApplicationName=DataGrip 2021.3.1 */ select * from t force index(a) where a between 10000 and 20000;
```

可以看到，Q1 扫描了 10 万行，显然是走了全表扫描，执行时间是 40 毫秒。Q2 扫描了 10001 行，执行了 21 毫秒。也就是说，我们在没有使用 force index 的时候，MySQL 用错了索引，导致了更长的执行时间。

这个例子对应的是我们平常不断地删除历史数据和新增数据的场景。

## 10.1 优化器的逻辑

选择索引是优化器的工作

优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。

扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

## **10.2 扫描行数如何判断**

MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。

这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。

举个例子，有两个字段，一个是布尔型值有0,1两种情况，一个是枚举型有10个枚举值。分别在两个字段上建索引，布尔型字段索引会把数据分成两部分，枚举型会把数据分成十份，根据索引查找的时候，布尔型选择了一个排除了一半，枚举型选一个会排除9/10，所以枚举型区分度更好

我们可以使用 show index 方法，看到一个索引的基数。如图 4 所示，就是表 t 的 show index 的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且其实都不准确。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112071445860.png)

**MySQL 是怎样得到索引的基数的呢？**

为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”

采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。

而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。

在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：

* 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。
* 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。

由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。

你可以从图 中看到，这次的索引统计值（cardinality 列）虽然不够精确，但大体上还是差不多的，选错索引一定还有别的原因。

其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112071451182.png)

rows 这个字段表示的是预计扫描行数。

其中，Q1 的结果还是符合预期的，rows 的值是 104620；但是 Q2 的 rows 值是 37116，偏差就大了。而图 1 中我们用 explain 命令看到的 rows 是只有 10001 行，是这个偏差误导了优化器的判断。

如果使用索引 a，每次从索引 a 上拿到一个值，都要回到主键索引上查出整行数据，这个代价优化器也要算进去的。

而如果选择扫描 10 万行，是直接在主键索引上扫描的，没有额外的代价。

优化器会估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快。当然，从执行时间看来，这个选择并不是最优的。



使用普通索引需要把回表的代价算进去，在图 1 执行 explain 的时候，也考虑了这个策略的代价 ，但图 1 的选择是对的。也就是说，这个策略并没有问题。

所以冤有头债有主，MySQL 选错索引，这件事儿还得归咎到没能准确地判断出扫描行数。至于为什么会得到错误的扫描行数，这个原因就作为课后问题，留给你去分析了。

既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。我们来看一下执行效果。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072059815.png)

> 8.0没有测试出和这个问题，在seesion1 提交之后就正常了

在实践中，如果你发现 explain 的结果预估的 rows 值跟实际情况差距比较大，可以采用这个方法来处理

```mysql
select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 1;
```

从条件上看，这个查询没有符合条件的记录，因此会返回空集合。

在开始执行这条语句之前，你可以先设想一下，如果你来选择索引，会选择哪一个呢？

为了便于分析，我们先来看一下 a、b 这两个索引的结构图。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072101780.png)

如果使用索引 a 进行查询，那么就是扫描索引 a 的前 1000 个值，然后取到对应的 id，再到主键索引上去查出每一行，然后根据字段 b 来过滤。显然这样需要扫描 1000 行。

如果使用索引 b 进行查询，那么就是扫描索引 b 的最后 50001 个值，与上面的执行过程相同，也是需要回到主键索引上取值再判断，所以需要扫描 50001 行。

如果使用索引 a 的话，执行速度明显会快很多。那么，下面我们就来看看到底是不是这么一回事儿。

```mysql

mysql> explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;
```

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072104712.png)

可以看到，返回结果中 key 字段显示，这次优化器选择了索引 b，而 rows 字段显示需要扫描的行数是 50198。

1. 扫描行数的估计值依然不准确；
2. 这个例子里 MySQL 又选错了索引。



## 10.3 索引选择异常和处理

其实大多数时候优化器都能找到正确的索引，但偶尔你还是会碰到我们上面举例的这两种情况：原本可以执行得很快的 SQL 语句，执行速度却比你预期的慢很多

> 使用错了索引。
>
> 1. 可以使用force index(key)进行校正。
> 2. 通过修改sql语句诱导优化器选择正确的索引，因为优化器选择索引会考虑三个因素，扫描行数、临时表和排序。
> 3. 重新建立一个更合适的索引。

一种方法是，采用 force index 强行选择一个索引。MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。

刚开始分析时，我们认为选择索引 a 会更好。现在，我们就来看看执行效果：

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072113194.png)

可以看到，原本语句需要执行 2.23 秒，而当你使用 force index(a) 的时候，只用了 0.05 秒，比优化器的选择快了 40 多倍。

也就是说，优化器没有选择正确的索引，force index 起到了“矫正”的作用

不过很多程序员不喜欢使用 force index，一来这么写不优美，二来如果索引改了名字，这个语句也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。

但其实使用 force index 最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现的，所以开发的时候通常不会先写上 force index。而是等到线上出现问题的时候，你才会再去修改 SQL 语句、加上 force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不够敏捷。

所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢？

既然优化器放弃了使用索引 a，说明 a 还不够合适，所以**第二种方法就是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引**。比如，在这个例子里，显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。

 

我们来看看改之后的效果

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072204120.png)

之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。

现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。

当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有 limit 1，因此如果有满足条件的记录， order by b limit 1 和 order by b,a limit 1 都会返回 b 是最小的那一行，逻辑上一致，才可以这么做。

```mysql
mysql> select * from (select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 100)alias limit 1;
```

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112072206028.png)

在这个例子里，我们用 limit 100 让优化器意识到，使用 b 索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。<!--因为b不用排序，又有limit 1，从5w里只要找到一条就可以返回了，如果选择a，因为要排序，就要扫完1000条，然后才能排序，这成本明显太大，所以选择了b。但如果是limit 100，选择b，虽然不用排序，但找到第一条记录后，还要向后查询，看后面有没有满足条件的100个记录，从5w中找100个的成本就大于从1000找100个的成本了，所以选择a。其实limit 20就会选择a了-->

**第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。**

不过，在这个例子中，我没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较少，尤其是经过 DBA 索引优化过的库，再碰到这个 bug，找到一个更合适的索引一般比较难。

如果我说还有一个方法是删掉索引 b，你可能会觉得好笑。但实际上我碰到过两次这样的例子，最终是 DBA 跟业务开发沟通后，发现这个优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。

## FAQ

**为什么经过这个操作序列，explain 的结果就不对了？**

delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。

但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。

这样，索引 a 上的数据其实就有两份。

# 十一、怎么给字符串字段加索引

现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天要讨论的问题。

> **1.为什么需要优化字符串上的索引 如果字符串较长，**
>
> 索引字段占用内存空间大，B+树高度较高，这样查询IO次数较多，耗时长。 个人认为未出现性能瓶颈，不需要过度优化，全字段索引也ok。 
>
> **2.优化方法及优缺点**
>
> 方法一：前缀索引概念：在建立索引的时候指定索引长度，且该长度的字段区分度高 
>
> * 优点： a. 相比全字段索引，每页存储的索引更多，查询索引IO次数少，效率高。即既节省了内存空间，又提高了查询效率。
>
> *  缺点： a. 指定索引长度的区分度低的话，扫描主键索引次数就会多，效率低； b. 不会使用覆盖索引，即使索引长度定义为全字段，也会去主键索引查询 适用场景：前缀索引区分度高 
>
> 方法二：倒序存储 概念：
>
> 字段保存的时候反序存储
>
> *  优点：同前缀索引 
> * 缺点： a.只适用于等值查询，不适用于范围、模糊查询。 b.每次保存、查询时需调用reverse()函数； c.若后缀索引区分度低，扫描行数会增多。 适用场景：索引字段后缀区分度高，前缀区分度低。 
>
> 方法三：添加hash字段，作为索引 概念：在表中添加一个hash字段并加索引，用于存储索引字段的哈希值如使用crc32()哈希函数，每次查询时先计算出字段的hash值，再利用hash字段查询。可能存在hash冲突，所以where需要加索引字段的等值条件。 
>
> 优点：哈希函数冲突概率低的话，平均扫描行数接近1。
>
>  缺点：只适用于等值查询，不适用于范围、模糊查询。 适用场景：只适用于等值查询，不适用范围查询或模糊查询。

假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：

```mysql

create table SUser(
ID bigint unsigned primary key,
email varchar(64), 
... 
)engine=innodb; 
```

由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：

```mysql
 select f1, f2 from SUser where email='xxx';
```

我们可以知道，如果 email 这个字段上没有索引，那么这个语句就只能做全表扫描。

同时，MySQL 是支持`前缀索引`的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。

比如，这两个在 email 字段上创建索引的语句：

```mysql
alter table SUser add index index1(email);
# 前缀索引，抛开唯一性获得索引空间上的优化，但是也会导致回表次数增多
alter table SUser add index index2(email(6))
```

第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串；而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。

那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图 2 和 3 所示，就是这两个索引的示意图。

![email 索引结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112121629239.jpg)

![email(6) 索引结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202112121629520.jpg)

从图中你可以看到，由于 email(6) 这个索引结构中每个邮箱字段都只取前 6 个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。

但，这同时带来的损失是，**可能会增加额外的记录扫描次数。**<!--前缀索引还不能使用覆盖索引-->

接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。

```mysql
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

如果使用的是 index1（即 email 整个字符串的索引结构），执行顺序是这样的：

1. 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值；
2. 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；
3. 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。

这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。

如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的：

1. 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；
2. 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃；
3. 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；
4. 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。

过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。

在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。

**通过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。**

但是，对于这个查询语句来说，如果你定义的 index2 不是 email(6) 而是 email(7），也就是说取 email 字段的前 7 个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到 ID2，只扫描一行就结束了。

**也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

于是，你就有个问题：当要给字符串创建前缀索引时，有什么方法能够确定我应该使用多长的前缀呢？

实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。

首先，你可以使用下面这个语句，算出这个列上有多少个不同的值：

```mysql
 select count(distinct email) as L from SUser;
```

然后，依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引，可以用这个语句：

```mysql
select 
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;
```

当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。

## 11.1 前缀索引对覆盖索引的影响

前面我们说了使用前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止如此，我们再看一下另外一个场景。

```mysql
select id,email from SUser where email='zhangssxyz@xxx.com';
```

与前面例子中的 SQL 语句

```mysql
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

相比，这个语句只要求返回 id 和 email 字段。

所以，如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。

即使你将 index2 的定义修改为 email(18) 的前缀索引，这时候虽然 index2 已经包含了所有的信息，但 InnoDB 还是要回到 id 索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。

也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。<!--前缀索引的目的并不是在二级索引树上保存更多数据，也做不到减少回表的次数，应该是前缀索引能够减少索引字段长度，节省空间，当和主键联合的时候，无法做到覆盖索引，需要去回表查询，从而导致覆盖索引的优化无法使用，必须要回表，然后确定前缀索引字段的正确性-->

前缀索引肯定不能减少回表次数啊 反而是会 >= 非前缀索引的回表次数。 前缀索引只是减少的数据大小，一个数据页16k可以存储更多的信息，降低树高，减少io次数。

## 11.2 其他方式

对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？

比如，我们国家的身份证号，一共 18 位，其中前 6 位是地址码，所以同一个县的人的身份证号前 6 位一般会是相同的。

假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为 6 的前缀索引的话，这个索引的区分度就非常低了。

按照我们前面说的方法，可能你需要创建长度为 12 以上的前缀索引，才能够满足区分度要求。

但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。<!--相同的数据页放下的索引值越少，索引树层级会变高，索引搜索效率也会降低-->

那么，如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理方法呢？这种方法，既可以占用更小的空间，也能达到相同的查询效率。

**第一种方式是使用倒序存储。**如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：

```mysql
 select field_list from t where id_card = reverse('input_id_card_string');
```

由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。当然了，实践中你不要忘记使用 count(distinct) 方法去做个验证。

**第二种方式是使用 hash 字段。**你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。

```mysql
alter table t add id_card_crc int unsigned, add index(id_card_crc);
```

然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。

```mysql
select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string'
```

<!--CRC32的基本特征： #1.CRC32函数返回值的范围是0-4294967296（2的32次方减1) #2.相比MD5，CRC32函数很容易碰撞 CRC32的使用场景： 由上述两个基本特性可知，MySQL CRC32 生成整型结果使用用bigint存储，而MD5需要varchar来存储。但是CRC32很容易碰撞-->

<!--在数据库版本满足要求的情况下，最好是使用crc64()，可以降低碰撞几率。-->

这样，索引的长度变成了 4 个字节，比原来小了很多。

**接下来，我们再一起看看使用倒序存储和使用 hash 字段这两种方法的异同点。**

首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样地，hash 字段的方式也只能支持等值查询。<!--倒序和hash都将数据打散，数据没有按照原有字符串的顺序排列。在进行范围查询时，需要进行顺序扫描，此时的扫描范围就会是全表。-->

它们的区别，主要体现在以下三个方面：

1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。
2. 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。
3. 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。

11.3 小结

1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。

## FAQ

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。

系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？



由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面 6 位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是 @gamil.com，因此可以只存入学年份加顺序编号，它们的长度是 9 位。而其实在此基础上，可以用数字类型来存这 9 位数字。比如 201100001，这样只需要占 4 个字节。其实这个就是一种 hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性。

# 十二、为什么我的MySQL会抖一下

> 平时的工作中，不知道你有没有遇到过这样的场景，一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。

## 12.1 SQL语句为什么会变慢

InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写完 redo log 后，就返回给客户端，本次更新成功。

做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。

掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。在这个 flush 操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**<!--每个表都是一个ibd文件, 每个文件都是分成n个16kb的page. page是IO的基本单位, 也就是从硬盘到内存每次都载入一个page. 所以用到的page既在内存也在硬盘ibd文件里. 在内存page上写写改改后, 这个page没写回ibd文件, 就成了脏页-->

不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。

接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账 10 文，这次又要赊 9 文。

![](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202202280954270.jpeg)

回到文章开头的问题，你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。

## 12.2 出发flush的场景

1. **redo log满**

第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。这个场景，对应的就是 InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202202280958868.jpg)

checkpoint 可不是随便往前修改一下位置就可以的。比如图 2 中，把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’之间就是可以再写入的 redo log 的区域。

2. **需要加载新页仅磁盘的时候，内存页不够用，要淘汰的页是脏页，此时需要刷盘** 

第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。

这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。

你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿 redo log 出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态：

* 一种是内存里存在，内存里就肯定是正确的结果，直接返回；
* 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。

3. **mysql认为自己空闲时。**

第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。

这种场景，对应的就是 MySQL 认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。<!--innodb技术内幕里面讲的更详细，master thread 在判断过去1s 的io次数来判断数据库是否忙碌，然后再决定是否刷新脏页-->

4. **正常关闭数据库时** 

第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。这种场景，对应的就是 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

## 12.3 四种场景下flush出发对性能的影响。

其中，第三种情况是属于 MySQL 空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题。所以这里，我们主要来分析一下前两种场景下的性能问题。

第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。

也就是说，更新数据就一套程序，没有备用方案； 更新redo log是必须的一个节点，redo log满了只能排队等待redo log刷出空闲的位置； 我想什么时候redo log才会被写满呢？ 生产的速度远大于消费的速度； 数据库出问题了，cpu和io资源被别的地方大量占用

第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，

* mysql的一个核心理念就是有内存就会用内存，包括读写数据、log（redo）等等的执行动作，因此buffer pool实际是作为一个内存管理器的理念存在的。

----

**缓冲池中的内存页有三种状态：**

第一种是，还没有使用的；

第二种是，使用了并且是干净页；

第三种是，使用了并且是脏页。

在innodb的内存buffer pool中，用 free list 来维护未使用页，用flush list维护脏页，用lru list来维护使用页

InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。<!-- 读入的数据页不在内存中，就会从磁盘读取，如果内存中的FLUSH LIST不空闲，则flush 脏页，如果FLUSH LIST 空闲，则在LRU LIST 上采用LRU算法淘汰“冷数据”-->

而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。

**所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：**

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。

所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。

## 12.4 InnoDB 刷脏页的控制策略

首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。

这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：

```
show global variables like 'innodb_io_capacity'
# 测试随机读写命令
fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest
```

**案例：刷脏参数不合理，导致系统卡顿**

其实，因为没能正确地设置 innodb_io_capacity 参数，而导致的性能问题也比比皆是。之前，就曾有其他公司的开发负责人找我看一个库的性能问题，说 MySQL 的写入速度很慢，TPS 很低，但是数据库主机的 IO 压力并不大。经过一番排查，发现罪魁祸首就是这个参数的设置出了问题。

他的主机磁盘用的是 SSD，但是 innodb_io_capacity 的值设置的是 300。于是，InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。

<!--innodb_io_capacity的默认值为200，所以如果主机磁盘用的是SSD，大家建议改为20000.-->

虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看 InnoDB 怎么控制引擎按照“全力”的百分比来刷脏页。

**如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？**

1. 脏页比例 

2. 脏页刷盘速度 

3. 刷新相邻页面策略 （bufferpool脏页比例 或 redolog 都可能成为读写sql的瓶颈） 
   1. 脏页比例默认75%，一定不要让其接近75% innodb_max_dirty_pages_pct =Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 
   2. 刷脏页速度 innodb_io_capacity定义的能力乘以R%来控制刷脏页的速度
   3. innodb_flush_neighbors=0（不开启脏页相邻淘汰） （对于机械硬盘顺序读写会有提升，ssd无提升。iops普通机械硬盘只有几百，ssd有上千，可以不开启） 
   4. 避免大量刷脏页，脏页flush可能会产生内存抖动

这个问题可以这么想，如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是 redo log 写满。

所以，InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。

InnoDB 会根据这两个因素先单独算出两个数字。

参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字，计算这个数字的伪代码类似这样：

```c
F1(M){ if M>=innodb_max_dirty_pages_pct then return 100; return 100*M/innodb_max_dirty_pages_pct;}
```

InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。F2(N) 算法比较复杂，你只要知道 N 越大，算出来的值越大就好了。<!--直接刷脏页是不会动redolog的，等后续应用redolog的时候，会根据LSN 的大小来判断这个页有没有应用到这条log Logging Sequence Number-->

然后，根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。<!--直接刷脏页是不会动redolog的，等后续应用redolog的时候，会根据LSN 的大小来判断这个页有没有应用到这条log-->

<!--两个值，一个描述的是迫近设置的最大脏页比率的程度；另一个描述的远离checkpoint的程度，取较大者来进行计算，作为百分比与io capacity 值乘，获取当前的脏页刷取速度。 也就是说 脏页比越高（其实是越靠近max值） 或离chdckpoint越远，刷取速度就可能越快-->

上述的计算流程比较抽象，不容易理解，所以我画了一个简单的流程图。图中的 F1、F2 就是上面我们通过脏页比例和 redo log 写入速度算出来的两个值。

![图 3 InnoDB 刷脏页速度策略](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202202281102684.png)

现在你知道了，InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。

<!--其实说到底影响性能的就是IO，和内存使用情况。设计者千方百计的减少IO次数，合理利用内存。采用redo log来顺序的把更新纪录写到磁盘，让操作尽可能的在内存中发生。但最终的IO和内存占满对性能的影响还是要面临的，就算是错峰操作也避免不了一些极端的情况，既然避免不了，那就尽可能的选择更好的时机更顺利的操作去完成这件事。-->

要尽量避免这种情况，你就要合理地设置 innodb_io_capacity 的值，并且**平时要多关注脏页比例，不要让它经常接近 75%。**

**脏页比例获取**

```mysql
# 其中，脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令参考下面的代码：

mysql> select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
select @a/@b;
```

接下来，我们再看一个有趣的策略。

一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。

在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。

找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。<!--找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。-->

而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。

在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。

## 12.5 小结

今天这篇文章，延续第 2 篇中介绍的 WAL 的概念，解释了这个机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

<!--写redo log是直接写进redo log就OK,所以是顺序写；如果要直接更新到数据页，就要找到具体位置，所以是随机写；感谢楼主和楼上-->

但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。在文章里，我也给你介绍了控制刷脏页的方法和对应的监控方式。

## 12.6 思考题

**一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。但如果你在配置的时候不慎将 redo log 设置成了 1 个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？**

1. 把相对应的数据页中的脏页持久化到磁盘,checkpoint往前推

2. 由于redo log还记录了undo的变化,undo log buffer也要持久化进undo log

3. 当innodb_flush_log_at_trx_commit设置为非1,还要把内存里的redo log持久化到磁盘上

4. redo log还记录了change buffer的改变,那么还要把change buffer purge到idb

   以及merge change buffer.merge生成的数据页也是脏页,也要持久化到磁盘
   上述4种操作,都是占用系统I/O,影响DML,如果操作频繁,会导致'抖'得向现在我们过冬一样。
   但是对于select操作来说,查询时间相对会更快。因为系统脏页变少了,不用去淘汰脏页,直接复用
   干净页即可。还有就是对于宕机恢复,速度也更快,因为checkpoint很接近LSN,恢复的数据页相对较少
   所以要控制刷脏的频率,频率快了,影响DML I/O,频率慢了,会导致读操作耗时长。

##  FAQ

**当内存不够用了，要将脏页写到磁盘，会有一个数据页淘汰机制（最久不使用），假设淘汰的是脏页，则此时脏页所对应的redo log的位置是随机的，当有多个不同的脏页需要刷，则对应的redo log可能在不同的位置，这样就需要把redo log的多个不同位置刷掉，这样对于redo log的处理不是就会很麻烦吗？（合并间隙，移动位置？）**
**另外，redo log的优势在于将磁盘随机写转换成了顺序写，如果需要将redo log的不同部分刷掉（刷脏页），不是就在redo log里随机读写了么？**

其实由于淘汰的时候，刷脏页过程不用动redo log文件的。

这个有个额外的保证，是redo log在“重放”的时候，如果一个数据页已经是刷过的，会识别出来并跳过。

**buffer pool里维护着一个脏页列表，假设现在redo log 的 checkpoint 记录的 LSN 为 10，现在内存中的一干净页有修改，修改后该页的LSN为12，大于 checkpoint 的LSN，则在写redo log的同时该页也会被标记为脏页记录到脏页列表中，现在内存不足，该页需要被淘汰掉，该页会被刷到磁盘，磁盘中该页的LSN为12，该页也从脏页列表中移除，现在redo log 需要往前推进checkpoint，到LSN为12的这条log时，发现内存中的脏页列表里没有该页，且磁盘上该页的LSN也已经为12，则该页已刷脏，已为干净页，跳过。**

---

**什么是lsn**

https://www.modb.pro/db/62466#:~:text=LSN%E5%85%A8%E7%A7%B0%E6%98%AF%EF%BC%9Alog%20sequence,checkponit%E9%83%BD%E6%9C%89LSN%E6%A0%87%E8%AE%B0%E3%80%82

----

**innodb是如何知道一个页是不是脏页的，是有标记位还是通过redolog的ckeckpoint来确定的？**

每个数据页头部有LSN，8字节，每次修改都会变大。

对比这个LSN跟checkpoint 的LSN，比checkpoint小的一定是干净页

**flush、purge、merge****对应的是什么**

* flush 一般是说刷脏页，
* purge一般是指清undo log,
* merge一般是指应用change buffer

**访问某条记录时，存储引擎是如何判断这条记录所在的数据页是否在内存当中，这个查内存机制是如何实现的？**

每个页面有编号的。拿着编号去内存看，没有，就去磁盘

**如果直接将内存中的数据页刷入磁盘，那redo log中的数据怎么办，会去删除吗？如果是那redo log岂不就不连续了。如果不是的话当刷redo log时又该怎么判断此时硬盘中的数据已经是最新的数据了呢。**

 Redo log不用动，checkpoint 推进的时候再“丢弃”就可以了。丢弃的意思是，判断到数据页已经刷过，就不用再重放
