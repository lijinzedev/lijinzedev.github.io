---
title: Mysql45讲个人笔记与总结
top: false
cover: false
toc: true
mathjax: true
categories:
  - mysql
tags:
  - mysql
date: 2021-11-13 19:43:52
password:
summary:
---

# Mysql45

# 一、基础架构，SQL语句如何执行

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111131946742.png)

## 1.1 Server层

Server层：查询缓存、分析器、优化器、执行器等 以及所有内置的函数（eg:日期、时间、数学和加密函数等）所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

### 1.1.1 连接器

​			客户端连接到服务端，获取到==权限==等信息，然后在连接的有效时长内(==interactive_timeout==和==wait_timeout==参数控制， 5.7版本会断开可以自动重连)对sql进行处理。

* connect-timeout：连接过程中的等待时间
* wait-timeout：连接完成后，使用过程中的等待时间
* interactive-timeout：交换式……

`show processlist`

```mysql
# 通过命令可以去查看连接状态
show processlist
show variables like 'wait_timeout';
```

**注意：**

> 连接器会到权限表中查询拥有的权限，之后这个连接练得权限判断逻辑，都依赖于读到的权限，并且，连接在建立之后如果修改权限也不影响已经存在的连接，只有连接新建的时候，才会生效

**长链接与短连接**

> **长链接与短连接**是一种“行为”，比如连接完，执行一个查询，就断开，这是短连接；执行一个查询，不断开，下次查询还用这个连接，持续使用，就是长链接
>
> 换句话说，在不用连接池的时候，每次查询都要获取连接，查询完成之后调用close方法，这就是短链接，使用连接池之后，因为连接都被放入容器中，每次用完就放进去，而不是close，这种就是长连接

**mysql_reset_connection** 影响的会话状态相关的信息

> mysql_reset_connection()影响以下与会话状态相关的信息： ·回滚活跃事务并重新设置自动提交模式 ·释放所有的表锁 ·关闭或删除所有的临时表 ·重新初始化会话的系统变量值 ·丢失用户定义的变量设置 ·释放prepared语句 ·关闭handler变量 ·将last_insert_id()的值设置为0 ·释放get_lock()获取的锁 ·清空通过mysql_bind_param()调用定义的当前查询属性 返回值： 0表示成功，非0表示发生了错误。

#### FQA

**如何解决长链接所导致的内存占用过大MySQL异常重启问题（OOM）？**

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

2. 首先会判断查询缓存是否开启，如果已经开启，会判断sql是select还是update/insert/delete，对于select，尝试去查询缓存，如果命中缓存直接返回数据给客户端， 如果缓存没有命中，或者没有开启缓存， 会进入到下一步分析器。

### 1.1.2 查询缓存

> 查询缓存：mysql拿到一个查询后，先查询缓存，（缓存保存形式KV（K为查询的语句，V为查询的结果））

#### FQA

**为什么建议关闭缓存？**

> 失效频繁，而且对于更新压力大的数据库，命中率非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。==mysql8.0版本取消了查询缓存的整个功能模块。==

```mysql
# 关闭查询缓存
show variables like '%query_cache_type%';
set GLOBAL query_cache_type='OFF';
```

### 1.1.3 分析器

> 分析器进行语法分析(根据词法分析的结果和语法规则，判断是否满足Mysql语法)、词法分析，检查sql的语法顺序等得到==解析树==， 然后预处理器对解析树进一步分析，==验证数据表、字段是否存在==，通关之后sql进入下一步优化器，

### 1.1.4 优化器

> 优化器对sql执行计划分析，决定使用哪个索引；在一个语句有多表关联（join）时，决定各个表的连接顺序，得到最终执行计划，得到优化后的执行计划之后交给执行器。

执行器调用存储引擎api执行sql，得到响应结果， 将结果返回给客户端，如果缓存是开启状态， 会更新缓存。

### 1.1.5 执行器

> 执行器在执行的时候会先判断对于表T有没有查询权限，如果没有返回没有权限的错误(在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

**precheck验证权限**

> 权限验证不仅仅在执行器这部分会做，在分析器之后，也就是知道了该语句要“干什么”之后，也会先做一次权限验证。叫做precheck。而precheck是无法对运行时涉及到的表进行权限验证的，比如使用了触发器的情况。因此在执行器这里也要做一次执行时的权限验证。

**执行流程**

> 1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
> 2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
> 3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

FAQ

## 1.2 存储引擎层

负责数据的存储和提取，架构模式位==插件式==的支持InnoDB（默认）、MyISAM（不支持事务）、Memory（针对具体表，而非数据库）

InnoDB、MyISAM、Memory比较：：
InnoDB：支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（银行），
要求实现并发控制（售票），选择InnoDB。如果需要频繁更新、删除操作的数据库也选择InnoDB。因为支持
事务的提交和回滚
MyISAM：插入数据块、空间和内存使用比较低。如果表主要用于插入新记录和读出记录，选择MyISAM能实现处理高效率。
如果应用的完整性、并发性要求比较低，也可使用。
Memory：所有数据存在内存中，数据处理速度快，但不安全。如果需要很快的读写速度，对数据的安全性要求低，可选择Memory
它对表的大小有要求，不能建立太大的表。

# 二、日志系统，更新语句如何执行

**关于缓存**

​		对于MySQL8.0之前的版本，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这就是一般不建议使用查询缓存的原因。

## 2.1 redo log 重做日志

> 客户端执行DDL语句（create）/DML语句（insert，update，delete）/DCL语句（grant，revoke），数据库服务端执行的时候会涉及到 redo log（重做日志） 和 binlog（归档日志） 两个日志文件的更新。

**问题引出：**

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 ==IO 成本==、查找成本都很高。

**关于IO成本**

> IO成本就是寻址时间和上下文切换所需要的时间，最主要是用户态和内核态的上下文切换。我们知道用户态是无法直接访问磁盘等硬件上的数据的，只能通过操作系统去调内核态的接口，用内核态的线程去访问。 这里的上下文切换指的是同进程的线程上下文切换，所谓上下文就是线程运行需要的环境信息。 首先，用户态线程需要一些中间计算结果保存CPU寄存器，保存CPU指令的地址到程序计数器（执行顺序保证），还要保存栈的信息等一些线程私有的信息。 然后切换到内核态的线程执行，就需要把线程的私有信息从寄存器，程序计数器里读出来，然后执行读磁盘上的数据。读完后返回，又要把线程的信息写进寄存器和程序计数器。 切换到用户态后，用户态线程又要读之前保存的线程执行的环境信息出来，恢复执行。这个过程主要是消耗时间资源。 --来自《Linux性能优化实战》里的知识 SQL执行前优化器对SQL进行优化，这个过程还需要占用CPU资源

**关于随机IO**

> 随机 IO。 避免每次更新都要通过磁盘随机 IO 定位到记录位置。 将改动以顺序 IO 写到 redo log。可以使用组提交来批量更新。 类比，其实很多组件都是这么做的。 比如 Redis 的 Pipeline。以减少网络 IO。批量请求。

### 2.1.1 WAL技术

> MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。
>
> 详细的说：先写redo log到log buffer，具体内容就是针对哪个表空间的哪些页面做了哪些修改，然后log buffer中的日志内容会在某些时候写到redo日志文件中，比如事务提交时。至于为什么写redo日志会比刷新内存中的数据页到磁盘快，是因为服务器在启动时就已经给redo日志文件分配好了一块物理上连续的磁盘空间，每次写redo日志都是往文件中追加写，并没有寻址的过程。而修改过的数据页要刷新到磁盘的话，可能对应的磁盘空间并不是物理连续的，找起来费劲

​			当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并==更新内存==，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲（根据innodb_flush_log_at_trx_commit来决定）的时候做

**什么是更新内存**

> 更新内存的意思是先要把这一行记录从磁盘加载到内存中(buffer_pool),然后在内存中更新这个值。不会立即把最新值刷新到磁盘。
>
> 当需要更新的数据页在内存中时，就会直接更新内存中的数据页；不在内存中时，在可以使用 change buffer 的情况下，就会将更新操作记录到 change buffer 中，并将这些操作记录到 redo log 中；

---

### 2.1.3 redo log 图

> redo log 是循环写的，空间固定会用完，之后就去写磁盘，有刷页机制；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
>
> redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111210009122.png)

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

**落盘机制**

innodb_flush_log_at_trx_commit 变量控制日志缓冲区的内容如何写入和刷新到磁盘。innodb_flush_log_at_timeout 变量控制日志刷新频率(1s由log buffer刷盘一次）

> 落盘机制可以通过innodb_flush_log_at_trx_commit参数来控制：
>    当设置为1的时候，事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file on disk中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。
>    当设置为0的时候，事务提交时不会将log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到log file on disk中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。
>    当设置为2的时候，每次提交都仅写入到os buffer，然后是每秒调用fsync()将os buffer中的日志写入到log file on disk。

1. 后台线程定期会刷脏页
2. 清理LRU链表时会顺带刷脏页
3. redoLog写满会强制刷
4. 数据库关闭时会将所有脏页刷回磁盘
5. 脏页数量过多（默认占缓冲池75%）时，会强制刷

### 2.1.3 crash-safe

> redo log 是 InnoDB引擎所特有的，所以我们如果再使用InnoDB引擎创建表时，如果数据库发生异常重启，之前提交的记录都不会丢失。 InnoDB正因为有了 redo log(重做日志)，才有了 crash-safe 的能力（即使mysql服务宕机，也不会丢失数据的能力）。

数据库重启了，内存中的数据页没有同步到磁盘中，可以通过redo log日志恢复

## 2.2 binlog 归档日志

binlog（归档日志）是Server 层自己的日志。

 如何查看 binlog： https://zhuanlan.zhihu.com/p/33504555 

1. 搜索 log 名称：show variables like '%log_bin%'; 
2.  查看 log 内容：show binlog events in 'binlog.000009’ 或 使用命令行工具：mysqlbinlog binlog.000009

**关于binlog**

> binlog日志格式binlog日志有两种格式，分别为STATMENT、ROW，运行模式有三种，分别为STATMENT、ROW和MIXED。 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。 STATMENT 基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。 * 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能； * 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。 ROW 基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。 * 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题； * 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨 MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlogredo log

**为什么会有两份日志？**

> 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。
>
> binlog还不能去掉。 一个原因是，redolog只有InnoDB有，别的引擎没有。 另一个原因是，redolog是循环写的，不持久保存，binlog的“归档”这个功能，redolog是不具备的。 只有两份日记才有crash-safe功能 为什么binlog不能做到crash-safe？ 假如只有binlog，有可能先提交事务再写binlog，有可能事务提交数据更新之后数据库崩了，还没来得及写binlog。我们都知道binlog一般用来做数据库的主从复制或恢复数据库，这样就导致主从数据库不一致或者无法恢复数据库了。同样即使先写binlog再提交事务更新数据库，还是有可能写binlog成功之后数据库崩掉而导致数据库更新失败，这样也会导致主从数据库不一致或者无法恢复数据库。所以只有binlog做不到crash-safe。为了支持crash-safe，需要redolog，而且为了保证逻辑一致，事务提交需要两个阶段：prepare阶段和commit阶段。写redolog并落入磁盘(prepare状态)-->写binlog-->commit。commit的时候是不会落盘的。

**binlog为什么没有crash_safe的能力呢？**

> 不考虑mysql现有的实现，假如现在重新设计mysql，只用一个binlog是否可以实现cash_safe能力呢？答案是可以的，只不过binlog中也要加入checkpoint，数据库故障重启后，binlog checkpoint之后的sql都重放一遍。但是这样做让binlog耦合的功能太多。
>
> 并且写入方式的问题，binlog是追加写，crash时不能判定binlog中哪些内容是已经写入到磁盘，哪些还没被写入。而redolog是循环写，从check point到write pos间的内容都是未写入到磁盘的。

**binlog 与redolog的不同**

> 1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
> 2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
> 3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
> 4. 两者在更新方面也不一样redo log 分为 redo log buffer 和 redo log file，buffer 到 file 是通过 os buffer 写入，写入机制分别为【延迟写】: 每秒从 redo log buffer 写入到 os buffer 和 redo log file， 【实时写，实时刷】: 即无延时实时写入，【实时写，延迟刷】: 每次写入到 os buffer 后每秒刷到 redo log file， binlog是在事务提交后一次性写入

REDO的写盘时间会直接影响系统吞吐，显而易见，REDO的数据量要尽量少。其次，系统崩溃总是发生在始料未及的时候，当重启重放REDO时，系统并不知道哪些REDO对应的Page已经落盘，因此REDO的重放必须可重入，即REDO操作要保证幂等。最后，为了便于通过并发重放的方式加快重启恢复速度，REDO应该是基于Page的，即一个REDO只涉及一个Page的修改。 熟悉的读者会发现，数据量小是Logical Logging的优点，而幂等以及基于Page正是Physical Logging的优点，因此InnoDB采取了一种称为Physiological Logging的方式，来兼得二者的优势。所谓Physiological Logging，就是以Page为单位，但在Page内以逻辑的方式记录。举个例子，MLOG_REC_UPDATE_IN_PLACE类型的REDO中记录了对Page中一个Record的修改，方法如下： （Page ID，Record Offset，(Filed 1, Value 1) ... (Filed i, Value i) ... ) 其中，PageID指定要操作的Page页，Record Offset记录了Record在Page内的偏移位置，后面的Field数组，记录了需要修改的Field以及修改后的Value。

---

**执行器与InnoDB引擎执行update语句内部流程**

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的==数据页==本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。<!--操作系统层面读取磁盘的最小单位本来就是页，内存页、cache页和磁盘页都是一样大小，一般是4KB，但是Innodb使用B+树存储数据时，一个树节点即一次磁盘IO到内存是16KB，4倍关系。-->
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行==新数据==。<!--写在了内存中，innodb中的内存模型中有一个buffer pool的结构。innodb的存储结构，它分为内存结构和磁盘结构。-->
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。<!--1.binlog只有在commit的时候才会写入； 2.当prepare log 写入成功且binglog写入成功后发生crash，在mysql启动时候，会自动commit这个事物； 3.当prepare log写入成功，binlog写入失败，此时发生crash，mysql启动会自动回滚掉这个事物。即Binlog如果已经写入磁盘，那么redo log是prepare, 且binlog已经完整了，这时候崩溃恢复过程会认可这个事务，提交掉。 达到了“用binlog恢复的库跟原库逻辑相同” 这个要求-->

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111211530363.png)

**数据恢复步骤**

> 恢复的时候的大致步骤可能如下，摘取下来仅供做设计思想的参考：
>
>  **Step1.** 按顺序扫描redolog，如果redolog中的事务既有prepare标识，又有commit标识，就直接提交（先将磁盘页读入内存，再用 redo更新形成 dirty page，然后再刷盘） **Step2** . 如果redolog事务只有prepare标识，没有commit标识，则说明当前事务在commit阶段crash了，binlog中当前事务是否完整未可知，此时拿着redolog中当前事务的XID（redolog和binlog中事务落盘的标识），去查看binlog中是否存在此XID a. 如果binlog中有当前事务的XID，则提交事务（复制redolog disk中的数据页到磁盘数据页） b. 如果binlog中没有当前事务的XID，则回滚事务（使用undolog来删除redolog中的对应事务）

### 2.2.1 两阶段提交

> 将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

**怎样让数据库恢复到半个月内任意一秒的状态？**

通过定期的整库备份加上binlog的操作回放可以保证数据的安全性。因为binlog记录的是数据的逻辑操作（原始sql语句），而redolog是数据的物理操作日志，并且非innodb的引擎没有redolog。因此回放的时候回使用binlog进行回放

例如：某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

* 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
* 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

**注：**在重放binlog之前，需要将binlog中误删除的那个位置前的操作给删除掉，不然还是会执行误删除操作。等于前面的所有操作都白费了，也可以指定重放的位置，重放到误删除操作之前的position。

---

**为什么要两阶段提交？**

> 因为从 “两阶段提交”的执行流程看，“ binlog 成功，redo log prepare 失败”的场景， redo log 和 binlog 还是不一致的。 真正的“两阶段提交” 是指对 redo log 进行“两阶段提交”：先 prepare，再commit。 数据库 crash-重启后，会对记录对redo log 进行check
>
> 1. 如果 redo log 已经commit，则视为有效。
>
> 2. 如果 redo log prepare 但未commit，则check对应的bin log记录是否记录成功。 
>    1. bin log记录成功则将该prepare状态的redo log视为有效 
>    2. bin log记录不成功则将该prepare状态的redo log视为无效

本质上是因为 redo log 负责事务； binlog负责归档恢复； 各司其职，相互配合，才提供(保证)了现有功能的完整性； 现在 你非要破坏 其中一个log，完了，还妄想保证上述的功能，怎么可能呢？ 除非你从根本上 改写binlog，合并redo log 和 binlog 的 职责和功能！

redolog和binlog具有关联行，在恢复数据时，redolog用于恢复主机故障时的未更新的物理数据，binlog用于备份操作。每个阶段的log操作都是记录在磁盘的，在恢复数据时，redolog 状态为commit则说明binlog也成功，直接恢复数据；如果redolog是prepare，则需要查询对应的binlog事务是否成功，决定是回滚还是执行。

简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 2.2.2 小结

> redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。<!--innodb_flush_log_at_trx_commit={0|1|2} # 指定何时将事务日志刷到磁盘，默认为1。 0表示每秒将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。 1表示每事务提交都将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。 2表示每事务提交都将"log buffer"同步到"os buffer"但每秒才从"os buffer"刷到磁盘日志文件中。-->

> sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

## 2.3 FAQ

**全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？**

一天一备和一周一备： - 好处：最长恢复时间更短。 一天一备——全备+这一天0点到当前时间的binlog (如果每次0点全备) 一周一备——全备+周一到当前时间的binlog (如果每周周一0点全备) - 代价：一天一备，频繁全量备份，需要消耗大量存储空间

在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。

一周一备最坏情况就要应用一周的 binlog 了。

系统的对应指标就是  RTO（恢复目标时间）。

当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间增加磁盘压力，所以这个 RTO 是成本换来的，就需要你根据业务重要性来评估了。

---

**数据库备份策略**

备份就是救命药加后悔药，灾难发生的时候备份能救命，出现错误的时候备份能后悔。事情都有两面性，没有谁比谁好，只有谁比谁合适，完全看业务情况和需求而定。一天一备恢复时间更短，binlog更少，救命时候更快，但是后悔时间更短，而一周一备正好相反。我自己的备份策略是设置一个16小时延迟复制的从库，充当后悔药，恢复时间也较快。再两天一个全备库和binlog，作为救命药,最后时刻用。这样就比较兼顾了。

---

**为什么先写日志后写磁盘还可以查到数据呢？**

因为数据已经在内存中了，不会去查找磁盘了

如果update到内存后，redolog/binlog写入之前，这个时刻【如果】有其他客户端B能读到的话（比如 10 更新为20了），从客户端来的角度来看的话，这个20是已经更新完成的了。然而如果redolog/binlog写入之前崩溃了，恢复时，这个更新肯定是要丢弃的（因为日志还没commit）；这时候之前的读就是有问题的。所以个人认为在commit之前，哪怕是更新到内存了，其他读取理应是看不到这个尚未commit的数据的（和事务隔离级别有关？）

---

**由于脏页导致的普通select超过30s**

所谓刷脏就是由于内存页和磁盘数据不一致导致了该内存页是“脏页”，将内存页数据刷到磁盘的操作称为“刷脏”。刷脏是为了避免产生“脏页”，主要是因为MySQL更新先写redo log再定期批量刷到磁盘的，这就导致内存页的数据和磁盘数据不一致，为了搞清楚为什么“刷脏”会导致慢查，我们先分析下redo log再哪些场景会刷到磁盘。
场景1：redo log写满了，此时MySQL会停止所有更新操作，把脏页刷到磁盘
场景2：系统内存不足，需要将脏页淘汰，此时会把脏页刷到磁盘
场景3：系统空闲时，MySQL定期将脏页刷到磁盘

可以想到，在场景1和2都会导致慢查的产生，根据文章提到的，redo log是可以循环写的，那么即使写满了应该也不会停止所有更新操作吧，其实是会的，文中有句话“粉板写满了，掌柜只能停下手中的活，把粉板的一部分赊账记录更新到账本中，把这些记录从粉板删除，为粉板腾出新的空间”，这就意味着写满后是会阻塞一段时间的。

那么问题来了，innodb存储引擎的刷脏策略是怎么样的呢？通常而言会有两种策略：全量（sharp checkpoint）和部分（fuzzy checkpoint）。全量刷脏发生在关闭数据库时，部分刷脏发生在运行时。部分刷脏又分为定期刷脏、最近最少使用刷脏、异步/同步刷脏、脏页过多刷脏。

---

**关于数据页**

MySQL的记录是以“页”为单位存取的，默认大小16K。也就是说，你要访问磁盘中一个记录，不会只读这个记录，而会把它所在的16K数据一起读入内

---

**如果两种log都写完成了，但是redolog没有写磁盘，物理机挂了，redolog在内存中就丢失了吧，再启动跟磁盘中的binlog就不一致，后期恢复数据的时候会有问题吗**

需要讲trx_commit设置成1

---

**redo log为什么要设计成环形的？环形缓冲区的设计又有什么好？**

可以避免新建、删除文件带来的抖动

# 三、事物隔离

## 3.1 隔离性与隔离级别 

## 3.2 事物的ACID属性

* **Atomicity（原子性）：**

    一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 

* **Consistency（一致性）：**

  在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。

*  **Isolation（隔离性）：**

  数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 

* **Durability（持久性）：**

  事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

> 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。

* **脏读:** 读到其他事务未提交的数据； 

* **不可重复读：**前后读取的记录内容不一致；

* **幻读：**前后读取的记录数量不一致。幻读也成幻行，指的是第一次查询和第二次查询返回的行集不一致。所以删除了一行数据也会导致幻读

## 3.3 事物的隔离级别

1. **读未提交RU：**一个事务还没提交时，它做的变更就能被别的事务看到。会造成“脏读”，“幻读”，“不可重复读取”。 
2. **读提交RC：**一个事务提交之后，它做的变更才会被其他事务看到。避免了“脏读”，“但不能避免""幻读“和”不可重复读取”。(是大多主流数据库默认的事务等级)
3. **可重复读RR：**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。避免了“脏读“和”不可重复读取“的情况，但不能避免“幻读”。（带来了更多的性能损失）
4. **串行化Seria：**顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。（最严格级别，事务串行执行，资源消耗最大）

**注意：**

> 在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，要记得将 MySQL 的隔离级别设置为“读提交”
>
> 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。
>
> 5.7版本就是@@tx_isolation，8.0以上是transaction_isolation

## 3.4 视图

数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

1. **RC级别:**   MVCC视图会在每一个语句前创建一个，所以在RC级别下，一个事务是可以看到另外一个事务已经提交的内容、因为它在每一次查询之前都会重新给予最新的数据创建一个新的MVCC视图。 

2. **RR级别:** MVCC视图在，视图是在第一个Select语句执行时创建的吧？，这个视图会一直使用，直到该事务结束。 这里要注意不同的隔离级别他们的一致性事务视图创建的时间点是不同的。 

   事务最开始是update语句时，这个时候还没创建视图，当事务询查到第一条查询语句才开始创建视图

3. **RU：**没有视图的概念，直接返回记录上的最新值

4. **串行化：** 直接用加锁的方式来避免并行访问。

## 3.5 事物隔离的实现

> 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。<!--意思就是除了记录变更记录，还会记录一条变更相反的回滚操作记录，前者记录在redo log，后者记录在undo log-->

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111231403770.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小

## 3.7 事务的启动方式

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。一致性视图是在执行第一个快照读语句时创建的；
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。
3.  start transaction with consistent snapshot; 如果你想要马上启动一个事务，可以使用 `start transaction with consistent snapshot `这个命令。  一致性视图是在执行 start transaction with consistent snapshot 时创建的。
4. commit work and chain 将提交和开启事务用一条语句完成可以弥补，减少交互，来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

> 很多语言的第三方库都是使用连接池维持可复用的长连接来保持与mysql的链接，需注意，长连接并不意味着长事务，需要判断是否将autocommit设置成了0.

建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。

如果纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。

## 3.8 长事务查询

information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

## FAQ

**回滚日志何时删除?**

> 在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。
>
> 回归日志生命周期 在开始时创建 提交后不一定删除 只有在提交后且没有比当前更早的事务时 回滚日志才会被删除 所以尽量不要使用长事务

**为什么不要使用长事务?**

 长事务意味着系统里面会存在很老的事务视图。 由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。

**如何避免长事务**

首先，从应用开发端来看：

1. 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。

   ```bash
   一、查询日志开启 
   方法一： 　　
   #设置路径
   mysql>set global general_log_file='/tmp/general.lg';  　
   # 开启general log模式 
   mysql>set global general_log=on; 　　
   # 关闭general log模式 命令行设置即可,无需重启 在general log模式开启过程中，所有对数据库的操作都将被记录 general.log 文件 
   mysql>set global general_log=off; 
   方法二： 也可以将日志记录在表中 
   set global log_output='table' 运行后,可以在mysql数据库下查找 general_log表 
   二、查询日志关闭 查看是否是开启状态： 
   mysql> show global variables like '%general%'; 
   #  关闭查询日志
   mysql> set global general_log = off; 
   ```

   

2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。

3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

其次，从数据库端来看：

1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
2. Percona 的 pt-kill 这个工具不错，推荐使用；
3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

# 四、深入浅出索引一

> 索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本 500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。

## 4.1 B+树

哈希表能快速找到数据，但是不支持范围查找，有序数组支持范围查找，但是不支持随机插入，B+树俩者都支持。

### 4.1.1 MySQL的存储结构

#### 4.1.1.1 表存储结构

单位：表>段>区>页>行

在数据库中， 不论读一行，还是读多行，都是将这些行所在的页进行加载。也就是说存储空间的基本单位是页。

一个页就是一棵树B+树的节点，数据库I/O操作的最小单位是页，与数据库相关的内容都会存储在页的结构里。

#### 4.1.1.2 B+树索引结构

在一棵B+树中，每个节点为都是一个页，每次新建节点的时候，就会申请一个页空间

同一层的节点为之间，通过页的结构构成了一个双向链表

非叶子节点为，包括了多个索引行，每个索引行里存储索引键和指向下一层页面的指针

叶子节点为，存储了关键字和行记录，在节点内部(也就是页结构的内部)记录之间是一个单向的表

#### 4.1.1.3 B+树页节点结构

有以下几个特点

将所有的记录分成几个组， 每组会存储多条记录，

页目录存储的是槽(slot)，槽相当于分组记录的索引，每个槽指针指向了不同组的最后一个记录

我们通过槽定位到组，再查看组中的记录

页的主要作用是存储记录，在页中记录以单链表的形式进行存储。

单链表优点是插入、删除方便，缺点是检索效率不高，最坏的情况要遍历链表所有的节点。因此页目录中提供了二分查找的方式，来提高记录的检索效率。

#### 4.1.1.4 B+树的检索过程

我们再来看下B+树的检索过程

从B+树的根开始，逐层找到叶子节点。

找到叶子节点为对应的数据页，将数据叶加载到内存中，通过页目录的槽采用二分查找的方式先找到一个粗略的记录分组。

在分组中通过链表遍历的方式进行记录的查找。

#### 4.1.1.5 为什么要用B+树索引

数据库访问数据要通过页，一个页就是一个B+树节点，访问一个节点相当于一次I/O操作，所以越快能找到节点，查找性能越好。

B+树的特点就是够矮够胖，能有效地减少访问节点次数从而提高性能。

下面，我们来对比一个二叉树、多叉树、B树和B+树。

### 4.1.2 二叉树

二叉树是一种二分查找树，有很好的查找性能，相当于二分查找。

但是当N比较大的时候，树的深度比较高。数据查询的时间主要依赖于磁盘IO的次数，二叉树深度越大，查找的次数越多，性能越差。

最坏的情况是退化成了链表，如下图

为了让二叉树不至于退化成链表，人们发明了AVL树(平衡二叉搜索树)：任何结点的左子树和右子树高度最多相差1

### 4.1.3 多叉树

多叉树就是节点可以是M个，能有效地减少高度，高度变小后，节点变少I/O自然少，性能比二叉树好了

### 4.1.4 B树

B树简单地说就是多叉树，每个叶子会存储数据，和指向下一个节点的指针。

例如要查找9，步骤如下

我们与根节点的关键字 (17，35)进行比较，9 小于 17 那么得到指针 P1；

按照指针 P1 找到磁盘块 2，关键字为(8，12)，因为 9 在 8 和 12 之间，所以我们得到指针 P2；

按照指针 P2 找到磁盘块 6，关键字为(9，10)，然后我们找到了关键字 9。

### 4.1.5 B+树

B+树是B树的改进，简单地说是：只有叶子节点才存数据，非叶子节点是存储的指针；所有叶子节点构成一个有序链表

例如要查找关键字16，步骤如下

与根节点的关键字 (1，18，35) 进行比较，16 在 1 和 18 之间，得到指针 P1(指向磁盘块 2)

找到磁盘块 2，关键字为(1，8，14)，因为 16 大于 14，所以得到指针 P3(指向磁盘块 7)

找到磁盘块 7，关键字为(14，16，17)，然后我们找到了关键字 16，所以可以找到关键字 16 所对应的数据。

### 4.1.6 B+树与B树的不同：

B+树非叶子节点不存在数据只存索引，B树非叶子节点存储数据

B+树使用双向链表串连所有叶子节点，区间查询效率更高，因为所有数据都在B+树的叶子节点，但是B树则需要通过中序遍历才能完成查询范围的查找。

B+树每次都必须查询到叶子节点才能找到数据，而B树查询的数据可能不在叶子节点，也可能在，这样就会造成查询的效率的不稳定

B+树查询效率更高，因为B+树矮更胖，高度小，查询产生的I/O最少。

这就是MySQL使用B+树的原因，就是这么简单！

## 4.2 InnoDB 的索引模型

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

对于Innodb对应的N叉树大小，以InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。

MySql默认一个节点的长度为16K，一个整数（bigint）字段索引的长度为 8B,另外每个索引还跟着6B的指向其子树的指针；所以16K/14B ≈ 1170 参见https://blog.csdn.net/weixin_35871519/article/details/113303881

https://github.com/jeremycole/innodb_diagrams/blob/master/images/InnoDB_Structures.pdf

**关于 InnoDB 的表结构：**

1. 在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树。

2. 执行查询的效率，使用主键索引 > 使用非主键索引 > 不使用索引。 

3. 如果不使用索引进行查询，则从主索引 B+ 树的叶子节点进行遍历。

**每一个索引在 InnoDB 里面对应一棵 B+ 树。**

**例如：**

```mysql
create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;
insert into t(id,k) values(100,1),(200,2),(300,3),(500,5),(600,6);
```

![InnoDB 的索引组织结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111231611454.png)

> 二级索引的叶子节点存的就是主键的值，不是引用！根据这个值进行回表，再在聚簇索引里面直接从叶子节点里面拿到值，不需要进行一次IO去磁盘找。为啥存值不存引用（数据在磁盘的地址指针）？因为一旦发生页分裂或者页合并，就得去维护这个地址指针，更麻烦。

---

**基于主键索引和普通索引的查询有什么区别？**

主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

---

## 4.3 索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。

除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。

数据页参见https://www.cnblogs.com/zhuchangwu/p/14041410.html

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。

**适合业务字段做主键的场景**

1. 只有一个索引；

2. 该索引必须是唯一索引。  

这就是典型的 KV 场景。由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。

这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。

## FAQ

**重建索引问题**

如果你要重建索引 k，你的两个 SQL 语句可以这么写：

`alter table T drop index k;`

`alter table T add index(k);`

如果你要重建主键索引，也可以这么写：

`alter table T drop primary key;`

`alter table T add primary key(id);`

对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？

1. 直接删掉主键索引是不好的，它会使得所有的二级索引都失效，并且会用ROWID来作主键索引；
2. 看到mysql官方文档写了三种措施，第一个是整个数据库迁移，先dump出来再重建表（这个一般只适合离线的业务来做）；第二个是用空的alter操作，比如ALTER TABLE t1 ENGINE = InnoDB;这样子就会原地重建表结构（真的吗？）；第三个是用repaire table，不过这个是由存储引擎决定支不支持的（innodb就不行）。

**注意**

> 记录日志的表最好是分区表，历史数据清理可以直接drop分区	

---

**“N叉树”的N值在MySQL中是可以被人工调整的么？**

1， 通过改变key值来调整
N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
2， 改变页的大小
页越大，一页存放的索引就越多，N就越大。

数据页调整后，如果数据页太小层数会太深，数据页太大，加载到内存的时间和单个数据页查询时间会提高，需要达到平衡才行。

**没有主键的表，有一个普通索引。怎么回表？**

https://dev.mysql.com/doc/refman/5.6/en/innodb-index-types.html
5.6 文档
先找非空唯一索引；
如果没有，再用rowid 

**innodb B+树主键索引的叶子节点存的是什么?**

B+树的叶子节点是page （页），一个页里面可以存多个行

 因为存的不可能是“页” 这一逻辑概念 只能说这个叶结点大小等于innoDB里设置的页大小 或者说这个叶结点其实就是“页” 但存的是什么 那当然是数据 什么数据 当然是表中的行数据

**普通索引，为什么要用回表，直接指向数据行，数据页不行吗？**

第一、主键索引和普通索引，实际指向的都是一个页地址，在每个页中，主键索引页每行的value存的是实际数据，普通索引页每行的value是主键id。
第二、如果普通索引页每行的value是主键索引页地址，那么在发生页分裂、页合并、主键索引重建的时候，需要遍历所有的普通索引，查找涉及到的普通索引key进行更新。
第三、可以理解为：使用id值，相当于对主键索引与普通索引进行了解耦，主键索引页的变更不会影响到其他的普通索引。
第四、关于解耦带来的好处，与查询上带来的性能损失之间的定量分析，希望有其他同学补充
第五、可能还有其他的原因，欢迎其他同学补充

**1、如果插入的数据是在主键树叶子结点的中间，后面的所有页如果都是满的状态，是不是会造成后面的每一页都会去进行页分裂操作，直到最后一个页申请新页移过去最后一个值**

只会分裂它要写入的那个页面。每个页面之间是用指针串的，改指针就好了，不需要“后面的全部挪动

在Page 数据结构中，记录 Page 的头信息的File Header 字段中有 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 字段，通过这两个字段，来确定该页的上一页和下一页，实际上所有页通过两个字段可以形成一条双向链表的

**2、还有之前看到过说是插入数据如果是在某个数据满了页的首尾，为了减少数据移动和页分裂，会先去前后两个页看看是否满了，如果没满会先将数据放到前后两个页上，不知道是不是有这种情况**

对，减为了增加空间利用率

**为什么现在一般自增索引都设置为bigint**

因为现在很多业务插入数据很凶残，容易超过int 上限，实际上是建议设置bigint unsigned



**想问如果主键开始是int，自增不够了，改成bigint，内部该怎么处理，是新建表，还是要分裂**？



---

**索引只能定位到page，page内部怎么去定位行数据**

知识点:Page directory ,内存中利用二分查找

https://www.cnblogs.com/bdsir/p/8745553.html感觉这个很清楚

https://blog.csdn.net/cy973071263/article/details/104512020

---

**非聚集索引上为啥叶子节点的value为什么不是地址，这样可以直接定位到整条数据，而不用再次对整棵树进行查询**

这个叫作“堆组织表”，MyISAM就是这样的，各有利弊。你想一下如果修改了数据的位置的情况，InnoDB这种模式是不是就方便些，对于主键索引页分裂的场景， 就可能会导致主键记录的地址发生变化， 这时候需要更新每一个索引上面对主键记录地址的引用

---

**整张表的数据其实就是存在主键索引中的**

---

**什么情况下创建索引才有意义？有哪些限制？比如字段长度**

 有这个索引带来的查询收益，大于维护索引的代价，就该建😄 对于可能变成大表的表，实际上如果不建索引会导致全表扫描，这个索引就是必须的。

---

**如何查看索引占用多少空间？**

可以估算出来的，根据表的行数和索引的定义。

---

**查看索引数的结构，比如多少个层，多少节点？**

跟上一个一样。 如果要精确的，就要解数据文件，这个工具可以看看https://github.com/jeremycole/innodb_diagrams

---

**如何查看索引的利用率。比如我创建了一个索引，是否可以有记录这个索引被调用了多少次？**

 performance_schema.table_io_waits_summary_by_index_usage能看到一些信息

---

**关于innodb中自增索引的插入问题**

按照传统B+树的插入规则，即使是自增插入，当一个数据页满的时候，也是会引起页分裂的。但是innodb在这一块做了优化，即判断如果是自增插入且当前页已满的情况下，不改变原有页的结构，而是将新的数据放到一个新页中。
在innodb的实现中，为每个索引页面维护了一个上次插入的位置，以及上次的插入是递增/递减的标识。根据这些信息，innodb能够判断出新插入到页面中的记录，是否仍旧满足递增/递减的约束，若满足约束，则采用优化后的分裂策略；若不满足约束，则退回到传统的分裂策略。

---

**在插入数据的时候，主键类型为字符串，ID为uuid的形式，插入时会导致分裂吗？**

会，特别不建议uuid做主键

---

**联合索引在B+树中的存储方式**

https://mengkang.net/1302.html

# 五、深入浅出索引二

在下面这个表 T 中，如果执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？

```mysql

create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;
insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241743747.png)

1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；
2. 再到 ID 索引树查到 ID=300 对应的 R3；
3. 在 k 索引树取下一个值 k=5，取得 ID=500；
4. 再回到 ID 索引树查到 ID=500 对应的 R4；
5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。

在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。

## 5.1.1 覆盖索引

> 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**

<!--覆盖索引就是在这次的查询中，所要的数据已经在这棵索引树的叶子结点上了-->

**在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？**

身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。

## 5.1.2 最左前缀原则

B+ 树索引结构，可以利用索引的“最左前缀”，来定位记录。

https://cloud.tencent.com/developer/news/44861

**对（name，age）这个联合索引进行分析**

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241812273.jpg)

索引项是按照索引定义里面出现的字段顺序排序的。

"where name like ‘张 %’" 走索引

单独的年龄不走索引

> 如果单独的字段不能走联合索引，那么考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引

只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

**在建立联合索引的时候，如何安排索引内的字段顺序。**

评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

## 5.1.3 索引下推

【索引下推】Index Condition Pushdown，简称 ICP。 是Mysql 5.6版本引入的技术优化。旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能。 ------------------ 如果查询利用到了索引下推ICP技术，在Explain输出的Extra字段中会有“Using index condition”。即代表本次查询会利用到索引，且会利用到索引下推。 ------------------ 索引下推技术的实现——在遍历索引的那一步，由只传入可以利用到的字段值，改成了多传入下推字段值。

---

不符合最左查询的字段的查询过程

```mysql
 select * from tuser where name like '张%' and age=10 and ismale=1;
```

这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。

在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111241827519.jpg)

**总结：**

 1、覆盖索引：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据 

2、最左前缀：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符 

3、联合索引：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。

 4、索引下推：like 'hello%’and age >10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度

## FAQ

**关于日志处理**

分区表处理访问日志，drop分区

**优化器执行的顺序**

1.根据搜索条件找出所有可能用到的索引。
2.计算全表扫描的代价
3.计算其他索引扫描的代价
4.选出优化器计算出扫描成本最低的索引
（扫描代价涉及到I/O成本和CPU成本，innodb内部有一套自己的成本计算方法，简单举例比如：读取一个页到内存成本为1.0， 检测一条记录是否符合成本为0.2）

根据上面的逻辑，%j 这种条件使用了辅助索引。就是说，优化器认为“扫描辅助索引的代价比扫描聚集索引的代价要低”。

**为什么二级索引比主键索引小**

因为主键索引有额外的三个隐藏列，row_id，trx_id,roll_pointer,具体参考掘金小孩的mysql

**关于大数据表如何整理**

rename +新建表

**索引数据比数据大怎么处理**

删除数据的时候,当前删除索引被标记为被删除,改索引位置则可以被空间复用,当一页数据都被删除时页可以被复用,空间被复用但是磁盘系统表空间是未改变的,需要通过重建表,重建表的逻辑对数据紧凑排列来保存到临时文件中也就是online的逻辑对server端来说就是inplace逻辑

重建索引不行吗？

当没有开启`innodb_file_per_table`选项,导致所有的表(数据和索引)都存储在了一个文件中.
即使是使用了`optimize table`都无法释放空间.

可以参考这两篇文章:
https://stackoverflow.com/questions/1270944/mysql-innodb-not-releasing-disk-space-after-deleting-data-rows-from-table

https://stackoverflow.com/questions/3927690/howto-clean-a-mysql-innodb-storage-engine/4056261#4056261

**关于联合索引**

关于联合索引我的理解是这样的：比如一个联合索引(a,b,c)，其实质是按a,b,c的顺序拼接成了一个二进制字节数组，索引记录是按该字节数组逐字节比较排序的，所以其是先按a排序，再按b排序，再按c排序的，至于其为什么是按最左前缀匹配的也就显而易见了，没看过源码，不知道理解的对不对，希望老师指正。

给表创建索引时，应该创建哪些索引，每个索引应该包含哪些字段，字段的顺序怎么排列，这个问题没有标准答案，需要根据具体的业务来做权衡。不过有些思路还是可供参考的：
1.既然是一个权衡问题，没有办法保证所有的查询都高效，那就要优先保证高频的查询高效，较低频次的查询也尽可能的使用到尽可能长的最左前缀索引。可以借助pt-query-digest来采样统计业务查询语句的访问频度，可能需要迭代几次才能确定联合索引的最终字段及其排序。
2.业务是在演进的，所以索引也是要随着业务演进的，并不是索引建好了就万事大吉了，业务发生变化时，我们需要重新审视当初建的索引是不是还依然高效，依然能满足业务需求。
3.业内流传的有一些mysql 军规，其实这些并不是真正的军规，只是典型场景下的最佳实践。真正的军规其实就一条：高效的效满足业务需求。比如有个军规规定一个表上的索引数不超过5个，但如果我们现在有一些历史数据表、历史日志表，我们很明确的知道这些表上不会再有数据写入了，但我们的查询需求很多也很多样化，那我们在这些表上的索引数能不能超过5个？当然是没有任何问题的。当然关于这份军规还是要认真看一下的，但看的重点不是去记住它，而是要弄明白每一条军规它为什么这么规定，它这样规定是基于什么考虑，适用的场景和前提是什么，这些都弄明白了，你记不记得住这些军规都无所谓了，因为你已经把它溶化到了你的血液中，具体到自己的具体业务时游刃有余将是必然。

**公司有订单表，有些核心字段，比如订单号.时间(整型，时间戳，范围查找).订单状态（整型，6个值，可能in，可能=）.客户标识（整型，几百个值）.付款方式（整型，5个值），设备号（字符串，有权限需要in）,这6个字段后台都会用到查询筛选，而且不选的情况下条件就不传，按照联合索引最左原则，那么可能要建几十个索引，这是不可能的，这个表做了按月分表，数据量一张表大约1000万,不建立索引的话，后台选的条件没有建索引就会非常慢，强制最多只能查连续两个月的数据（union all），请有什么好的解决方案么？？？ **

得按照查询的模式，选最常见的来创建组合索引。比如如果时间+客户标识用得最多，就创建者两个的联合索引。

对于比较少用的条件，单独给这个字段建索引，然后查id出来跟别的字段的查询结果，在客户端取交集，也是一种思路。

**MySQL在执行一条SQL时，是如何选择使用哪个索引的。 possible keys有很多，根据什么选择用哪一个。**

索引统计信息、临时表成本、排序成本

**在不影响排序结果的情况下，在取出主键后，回表之前，会在对所有获取到的主键排序，请问是否存在这种情况？**

存在，在MySQL 里面叫做“MRR优化”

MRR（Multi Range Read) 是针对辅助索引的范围查询时， 会一次性读取多个主键的值，并进行排序后， 再一次性到主键索引进行回表。 这样可以将多次回表的随机IO转换成顺序IO，提升查询性能。

# 六、全局锁和表锁

MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作DML的时候，加 MDL 读锁；当要对表做结构变更操作DDL的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 如果对线上一个频繁DML操作的表做DDL如添加字段等操作，可能会导致死锁，使数据库连接资源被消耗完，导致数据库宕机。安全的解决方式是对表做DDL如添加字段时，设置执行语句的超时时间，写锁超时自动释放，不影响读锁。

MySQL 里面的锁大致可以分成

* 全局锁
* 表级锁
* 行锁

## 6.1 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是` Flush tables with read lock (FTWRL)`。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。<!--解锁unlock tables 可以给从库加读锁-->

**全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。**

---

**mysqldump 使用参数–single-transaction进行备份可以开启事务为什么需要FTWRL？**

致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。

single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。

**为什么不使用 set global readonly=true 而使用FTWRL？**

set global readonly=true   方式也可以让全库进入只读状态

一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。

二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

三是、在 slave 上 如果用户有超级权限的话 readonly 是失效的

业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，对里面任何一个表做加字段操作，都是会被锁住的。

## 6.2 表级锁

MySQL 里面表级别的锁有两种：一种是表锁（lock tables 表名 read write / unlock tables），一种是元数据锁（meta data lock，MDL)。

> 与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

### 6.2.1 lock tables 

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。

### 6.2.2 MDL

另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

> 元数据锁是server层的锁，表级锁，主要用于隔离DML（Data Manipulation Language，数据操纵语言，如select）和DDL（Data Definition Language，数据定义语言，如改表头新增一列）操作之间的干扰。每执行一条DML、DDL语句时都会申请MDL锁，DML操作需要MDL读锁，DDL操作需要MDL写锁（MDL加锁过程是系统自动控制，无法直接干预，读读共享，读写互斥，写写互斥）

因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。

* 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
* 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

### 6.2.3 为什么给一个小表加个字段，导致整个库挂了

> 给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表 t 是一个小表。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111251636592.jpg)

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。

之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。

如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

参考

https://blog.csdn.net/q2878948/article/details/96430129

### 6.2.4 如何安全地给小表加字段？

1. 解决长事务，事务不提交，就会一直占着 MDL 锁。要考虑先暂停 DDL，或者 kill 掉这个长事务。

2. 比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。

```mysql
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ... 
```

## 总结

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：

1. 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
2. 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。
3. 

MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## FAQ

**备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？**

**当备库用–single-transaction 做逻辑备份的时候，如果从主库的 binlog 传来一个 DDL 语句会怎么样？**

假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：

```mysql
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
/***开启一致性视图，一致性试图不包含表结构**/
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */
```

在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);

启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；

设置一个保存点，这个很重要（Q3）；

> 两阶段锁，事务回滚或者提交时，才会释放锁。Q6之后还需要备份其他表。备份期间会占用MDL读锁，设置回滚点，读完数据后，回滚释放锁。将锁的占用时间控制到最短。
>
> 备份过程中会有多个mdl锁，一个表一个（假设有tn个表）如果不设置savepoint，第一个表读完后，表1的dml锁还不会释放，会一直等tn的表读完，事务提交才会释放所有表的锁，所以时间会很长

show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。<!--ROLLBACK TO SAVEPOINT：目的是释放锁-->

DDL 从主库传过来的时间按照效果不同，下面有四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成

1. 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。
2. 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止；<!--获取表结构和后面的select是强相关的，但是到这时还没有加锁，因此，此时是可以执行dll语句的，当获取表结构后再select的时候发现表结构变更了就会报错，估计是为了备份创建的表结构和当前的结构不匹配导致的-->
3. 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。<!--已经持有了 MDL 的读锁，阻塞 binlog 的 DDL 操作。-->
4. 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。<!--从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。-->

------

**online ddl**

Online DDL的过程是这样的：
1. 拿MDL写锁
2. 降级成MDL读锁
3. 真正做DDL
4. 升级成MDL写锁
5. 释放MDL锁

**全局锁和表锁是Server层实现的**

**如何对大表添加字段**

使用Gh-ost

**关于表字段设计的建议**

建议全小写和下划线



# 七、行锁，如何减少行锁对性能的影响

> MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

## 7.1 两阶段锁

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

**如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：

1. 从顾客 A 账户余额中扣除电影票价；
2. 给影院 B 的账户余额增加这张电影票价；
3. 记录一条交易日志。

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

<!--需要进行死锁检测，即使只有100个线程事务，但死锁检测的复杂度是o(n^2)，会需要10000个数量级的检测，所以出现cpu消耗高，并发事务却没多少的情况-->

如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？

## 7.2 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261024389.jpg)

这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

* 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。

* 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数` innodb_deadlock_detect `设置为 on，表示开启这个逻辑。

在 InnoDB 中，`innodb_lock_wait_timeout `的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。<!--死锁的处理策略，破坏死锁的几个条件 互斥 占有并等待 不可剥夺 循环等待-->

你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

<!--过程示例：新来的线程F，被锁了后就要检查锁住F的线程（假设为D）是否被锁，如果没有被锁，则没有死锁，如果被锁了，还要查看锁住线程D的是谁，如果是F，那么肯定死锁了，如果不是F（假设为B），那么就要继续判断锁住线程B的是谁，一直走知道发现线程没有被锁（无死锁）或者被F锁住（死锁）才会终止-->

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

### 7.2.1 如何解决由这种热点行更新导致的性能问题呢？

* **可以将临时思死锁检测关闭** 但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
* **控制并发度** 比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。

* **对更新同一行的请求进行排队**，对于相同行的更新，在进入引擎前排队，这样在 InnoDB 内部就不会有大量的死锁检测工作了。
* **设计角度，将一行数据变成多行** 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。相当于子账户的概念，原理上就是分段汇总，Java原子类LongAdder也使用了这个原理。这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。

## 结论

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。

减少死锁的主要方向，就是控制访问相同资源的并发事务量。

## FAQ

**如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：**

* 第一种，直接执行 delete from T limit 10000;
* 第二种，在一个连接中循环执行 20 次 delete from T limit 500 order by id;
* 第三种，在 20 个连接中同时执行 delete from T limit 500。



你会选择哪一种方法呢？为什么呢？

如果一定要在这三个中选，肯定选第二个。 第一个事务太长，执行时间过长， 如果是主备形式的，影响数据同步的时间。 这么多数据如果回滚的话，那该是多痛苦的事情 加锁的时间过长，会造成锁超时的 第三个，很明显有并发问题，如果产生循环等待就是死锁了。 其实可以把id利用起来，20个连接，每个都删500个id，岂不更好。

第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。<!--从库回放relay log的时候也会锁很久（占用MDL锁，导致同步DDL的binlog延迟），导致主库同步过来的binlog阻塞，造成主从延迟-->

第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。

**关于死锁检测innodb_deadlock_detect，每条事务执行前都会进行检测吗？**

如果他要加锁访问的行上有锁，他才要检测。

1. 一致性读不会加锁，就不需要做死锁检测；

2. 并不是每次死锁检测都都要扫所有事务。比如某个时刻，事务等待状态是这样的：

  B在等A，
  D在等C，
  现在来了一个E，发现E需要等D，那么E就判断跟D、C是否会形成死锁，这个检测不用管B和A

---

**innodb行级锁是通过锁索引记录实现的。如果update的列没建索引，即使只update一条记录也会锁定整张表吗？**

如果列上没有索引，更新就是走主键索引树，逐行扫描满足条件的行，等于将主键索引所有的行上了锁，假如加上limit 1,扫描 主键索引树，直到找到第一条满足条件的行，扫描过的行都会被加上行锁。即为->如果update的列没建索引，即使只update一条记录也会锁定整张表

1. 当加上limit1之后 更新语句的执行流程是先去查询在去更新,也就是查询sql为 select * from t where name = "abc" limit 1 for update,相当于扫描主键索引找到第一个满足name="abc"的条件为止,此时锁的区间为(负无穷,当前行的id],如果在这个id之后的更新和插入时都不会锁住的,在这个id之前的更新和插入会阻塞,之后则不会阻塞

2. 如果不加limit 1的话,因为此时是整个主键索引全表扫描则整个表锁住了

3. 回表的行锁,比如字段name有普通索引,在更新操作时普通索引会锁住的同时,如果更新操作需要回表的话对应的主键索引也会存在锁(主键索引锁临界锁会退化为行锁),普通索引(间隙锁和行锁)

 如果隔离级别是rr的话，那就说明你不能出现不可重复读，所以当你在用没有索引的字段当where条件更新的话，并且没有加limit，那么mysql会扫描主键然后锁住所有的行，因为他就是要保证他在更新的时候，别的事务不能搞事情啊，要别人插入一条，或者更新了一条，恰好又满足我刚才那个where条件 ，那就违反了当前的隔离级别了，当然rc条件下，就无所谓了，反正我的隔离级别下会出现不可重复读，别人要要更新那请便，只要不搞我当前扫描到这行就行了（因为他加了行锁）。加了limit 亦然，rr下，我扫描到的这前多少行你不能给我搞事情，搞了就违反我的隔离级别，所以他锁住了前多少行，后面爱咋搞咋搞。rc也无所谓，也只是行锁，锁住那几行。

---

**不支持行锁的引擎，只能使用表锁，而表锁同一张表在同一时刻只能有一个更新。但是上节课讲的表级锁中的MDL锁，dml语句会产生MDL读锁，而MDL读锁不是互斥的，也就是说一张表可以同时有多个dml语句操作。感觉这两种说法有点矛盾**

不矛盾，MDL锁和表锁是两个不同的结构。

比如：
你要在myisam 表上更新一行，那么会加MDL读锁和表的写锁；
然后同时另外一个线程要更新这个表上另外一行，也要加MDL读锁和表写锁。

第二个线程的*MDL读锁是能成功加上*的，但是被表写锁堵住了。从语句现象上看，就是第二个线程要等第一个线程执行完成。

**update会持有读MDL。读和读不互斥。但是对于行锁来说。两个update同时更新一条数据是互斥的。这个是因为多种锁同时存在时，以粒度最小的锁为准的原因么？**

不是“以粒度最小为准”
而是如果有多种锁，必须得“`全部不互斥`”才能并行，只要有一个互斥，就得等。

**如果开启事务，然后进行死锁检测，如果发现有其它线程因为这个线程的加入，导致其它线程的死锁，这个流程能帮着分析一下么**

理论上说，之前没死锁，现在A加进来，出现了死锁，那么死锁的环里面肯定包含A，
因此只要从A出发去扫就好了

**如何在死锁发生时,就把发生的sql语句抓出来,？**

 show engine innodb status 里面有信息，不过不是很全…

> 一般中间件每次执行事务时，都会重置状态。比方说spring托管的事务会有重置代码，最明显的就是在spring事务切面的代码里，从getConnection()时就有类似的代码。对于发生死锁如何排查，一般是dba去做，其实mysql库表里是有的，开发一般是不给权限查的，这里只贴下自己写的，做抛砖引玉
> SELECT r.trx_id waiting_trx_id,
>    r.trx_mysql_thread_id waiting_thread,
>    r.trx_query waiting_query,
>    b.trx_id blocking_trx_id,
>    b.trx_mysql_thread_id blocking_thread,
>    b.trx_query blocking_query
> FROM information_schema.innodb_lock_waits w
>    INNER JOIN information_schema.innodb_trx b
>        ON b.trx_id = w.blocking_trx_id
>    INNER JOIN information_schema.innodb_trx r
>        ON r.trx_id = w.requesting_trx_id;

**在使用连接池的情况下,连接会复用.比如一个业务使用连接set sql_select_limit=1,释放掉以后.其他业务复用该连接时,这个参数也生效.请问怎么避免这种情况,或者怎么禁止业务set session？**

 5.7的reset_connection接口可以考虑一下

> 在业务里，比如使用mybatis使用数据库的连接池，一个事务获取一个连接时，另一个事务这时是获取不到这个连接的，只有一个事务执行完释放连接到连接池，其它事务才能获取到，而释放后的连接已经被恢复到获取时的状态，包括自动提交等设置

**双11的成交额,是通过redis累加的嘛？**

用redis的话，为了避免超卖需要增加了很多机制来保证。修改都在数据库里执行就方便点。前提是要解决热点问题

**关于使用char类型id索引失效问题**

原因是id 是char 类型，但是没有加单引号，所以没有进入id索引中，然后锁表了，所以导致死锁。

sql：delete from 表A where id =15426169754750004759008 STORAGEDB
(id是主键)

会出现隐式转换函数, ID索引会失效, 走全表扫描



# 八、事务隔离

```mysql
CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB;insert into t(id, k) values(1,1),(2,2);
```

![事务 A、B、C 的执行流程](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261434878.png)

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。<!--数据库在设计的时候，一定会考虑最大程度的支持事务之间的并发，那它一定会让锁的时间尽可能短-->

第一种启动方式，一致性视图是在执行第一个==快照读语句==（select ）时创建的；

第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。

在 MySQL 里，有两个“视图”的概念：

> 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。
>
> 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。
>
> 它没有物理结构，通过高低水位，数据版本号，undo日记来进行判断数据可见不可见，作用是事务执行期间用来定义“我能看到什么数据”。

## 8.1 “快照”在 MVCC 里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。<!--基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)
如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错(参考第六章)-->

InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261456872.png)

要获取旧版本的数据行时，可以通过最新版本的数据和最新版本到目的版本之间的 Undo Logs 计算出来，因为 Undo Logs 记录了每个对应版本对应行数据的值。 Undo Logs 中分为两种类型: 1. INSERT_UNDO（INSERT操作），记录插入的唯一键值； 2. UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。

事务执行之后，其他事务的更新对它虽然不可见，但是数据版本还是可见的，因为数据库实际上存储的是最新版本的数据。但是对于该事务来说，需要根据版本号以及Undo Logs计算出他需要的版本对应的数据

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

## 8.2 视图

**在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。**

<!--判断事物开启条件的一种是begin 并且执行第一个'操作 'InnoDB 表的语句，所以我第一句只执行非select的dml语句就会生成事务id，但是并不会生成一致性视图，在我要执行select的时候 其它事物可能已经创建。所以对于视图数组就会出现比当前事务id还大并且没有提交的事物，同理也会出现比当前事务还小且没有提交的事物。所以假如当前事务id为88 ，活跃数组就可能有[72,79,88,90,91] 不连续的原因也知道吧，就是因为在生成一致性视图的时候中间短事务早就提交了-->

事务id再当前读才会申请而UPDATE、DELETE、INSERT、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE是当前读。

数组里面事务 ID 的最小值记为低水位，==当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位==。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

**为什么数组中要保存当前所有事务ID**

> Innodb 要保证这个规则：事务启动以前所有还没提交的事务，它都不可见。
>
> 但是只存一个已经提交事务的最大值是不够的。 因为存在一个问题，那些比最大值小的事务，之后也可能更新（就是你说的98这个事务）
>
> 所以事务启动的时候还要保存“现在正在执行的所有事物ID列表”，如果一个row trx_id在这列表中，也要不可见。

而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111261518663.png)

<!--这张图应该是在申请事务ID时，事务的状态-->

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

比如，对于图 2 中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。

**InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。



**反向推到为什么不直接判断是否在视图数组中**

> 为什么不直接判断该事务ID是否存在于活跃事务数组中呢？这样还简便得多呀！
>    确实，这样实现起来简单，但效率不行。上述方式，虽然实现和理解起来稍微复杂，但效率很高，实现它的一个前提条件就是事务ID要根据创建时间有序递增，才能快速鉴别出一定可见和一定不可见的情况（情况1、2）。其实我觉得实现的原理思想和布隆过滤器的思想很相似，先快速鉴别出一定可见和不可见的数据（第1、2种），然后对于可能可见和可能不可见的数据（第3种）通过活跃数组进行精确判断。

**当开启事务时，需要保存活跃事务的数组（A），然后获取高水位（B）。在这两个动作之间（A和B之间）会不会产生新的事务？如果产生了新的事务，那么这个新的事务相对于当前事务就是可见的，不管有没有提交?**

> 代码实现上，获取视图数组和高水位是在事务系统的锁保护下做的，可以认为是原子操作，期间不能创建事务。

## 8.3 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**

**除了 update 语句外，select 语句如果加锁，也是当前读。**

下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。

```mysql

mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

## 8.4 小结

**InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。**

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

而当前读，总是读取已经提交完成的最新版本。

**为什么表结构不支持“可重复读”？**

这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。

**8.0已经把表结构放到InnoDB字典里，表结构支持可重复读。在mysqldump过程中修改表结构并不会导致程序终止了。**

## FAQ

### 减库存场景

当前库存：num=200
假如多线程并发：
AB同时开启事务，A先请求到行锁，

| 事务A                                                   | 事务B                                                   |
| ------------------------------------------------------- | ------------------------------------------------------- |
| start transaction;                                      | start transaction;                                      |
| select num from t where num>0;先查询当前库存值（num>0） |                                                         |
| update t set num=num-200; 库存减量                      | select num from t where num>0;先查询当前库存值（num>0） |
| Commit                                                  | update t set num=num-200; 库存减量                      |
|                                                         | Commit                                                  |


A：查询到num=200,做了库存减量成了0
B：事务启动后，查询到也是200，等 A 释放了行锁，B进行update，直接变成 -200
但是 B 查询时，时有库存的，因此才减库存，结果变成负的。

给 select 加读锁或者写锁吗 ？这种select 加锁，对业务影响大吗？

> 一开始Select 加锁虽然可以，但是会比较严重地影响并发数。
>
> 比较简单的做法是update语句的where 部分加一个条件： where nun >=200 .
> 然后在程序里判断这个update 语句的affected_rows,
> 如果等于1 那就是符合预期；
> 如果等于0，那表示库存不够减了，业务要处理一下去，比如提示“库存不足”

**思考**

如果使用乐观锁的话但在并发条件下更新，完全的cas会导致较多的更新失败。其实这里解决的是num不能被减成负，所以使用num>=200来保证就可以了。既保证业务正确，也提高并发性能，解决并发量适中的场景也只能这样. 如果并发太高, 那就是hot key problem, 后来的写入会timeout. 这就要用(写)缓存或者消息队列来辅助处理了.

# 九、普通索引与唯一索引如何选择

## 9.1 索引

### 9.1.1 概念

索引就好比一本书的目录，它会让你更快的找到内容，显然目录(索引)并不是越多越好，假如这本书1000页，有500也是目录，它当然效率低，目录是要占纸张的,而索引是要占磁盘空间的。

### 9.1.2 Mysql索引结构

Mysql索引主要有两种结构：B+树和hash.

* **hash:**hsah索引在mysql比较少用,他以把数据的索引以hash形式组织起来,因此当查找某一条记录的时候,速度非常快.当时因为是hash结构,每个键只对应一个值,而且是散列的方式分布.所以他并不支持范围查找和排序等功能.

* **B+树:**b+tree是mysql使用最频繁的一个索引数据结构,数据结构以平衡树的形式来组织,因为是树型结构,所以更适合用来处理排序,范围查找等功能.相对hash索引,B+树在查找单条记录的速度虽然比不上hash索引,但是因为更适合排序等操作,所以他更受用户的欢迎.毕竟不可能只对数据库进行单条记录的操作.

### 9.1.3 Mysql常见索引

* **主键索引(PRIMARY KEY()：**  是一种特殊的唯一索引，不允许有空值。

  ```mysql
  ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 
  ```

  

* **唯一索引(UNIQUE)：**与"普通索引"类似，不同的就是：索引列的值必须唯一，但允许有空值。

  ```mysql
   ALTER TABLE `table_name` ADD UNIQUE (`column`)
  ```

  

* **普通索引(INDEX)：**最基本的索引，没有任何限制

  ```mysql
    ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 
  ```

  

* **全文索引(FULLTEXT)：**仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时耗空间。

  ```mysql
   ALTER TABLE `table_name` ADD FULLTEXT ( `column` )
  ```

  

* **组合索引：**为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。

  ```mysql
  ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` )
  ```

  

## 9.2 性能分析





![InnoDB 的索引组织结构](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111271940806.png)

### 9.2.1 查询过程

假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。

* 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。
* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

**这种不同对查询的性能微乎其微**

InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。

因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。

当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。

但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。

### 9.2.2 更新过程

#### 9.2.2.1 change buffer

> insert buffer是对于非唯一辅助索引更新的优化。对于主键索引来说我们可以通过设置increment_id来实现顺序插入，但是对于辅助索引往往难以保证更新的顺序性，这样可能会导致每次对于辅助索引insert都是离散的，需要遍历整张表找到插入位置，从IO角度来说效率低，而且会将无用页读入内存占用空间，所以使用insert buffer来实现对于索引延时更新，当发生insert的时候，如果索引页在内存中，则更新，不再内存中则将更新记录到 insert buffer中，当索引页被读入到内存的时候再执行 merge 操作。 后面的innodb版本，不仅支持insert，同时还支持 update ，delete操作所以称为change buffer！！！

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。<!--这里说的数据页，指的是二级索引树的数据页，并不是聚簇索引即主键树的数据页。 如涉及到索引字段的更新，也是要更新对应的索引数据的，而索引树对应的数据页不在内存中，则changeBuffer会先保存这个数据，之后会和对应的数据页进行merge过程。 redoLog也是会记录这一动作的，所以更新对应索引树的数据不会丢失。-->

需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。<!--change buffer可以看成也是一个数据页，需要被持久化到 系统表空间（ibdata1），以及把这个change buffer页的改动记录在redo log里，事后刷进系统表空间（ibdata1）。-->

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

**为什么要使用change buffer ？**

首先要明确概念： 

1. mysql数据存储在主键索引树的叶子节点。 

2. 普通索引和唯一索引也都有自己的索引树，树的叶子节点存储的是主键ID。

3. 做更新操作（插入，更新，删除）会同时更新所有的索引树结构。---------insert：主键索引树和唯一建索引树的肯定都要更新，肯定是无法用到change buffer的；但是普通索引树的更新，是可以使用change buffer的。 update：只要涉及到相关字段更新，就要同时更新相应的索引树。道理同上。 【显然，insert操作的影响更大，如果有多个唯一索引，insert对内存命中率会有极大影响】 


1. 减少读磁盘：仅仅是减少的是对二级普通索引页的读磁盘操作，而对于其他类型的页(唯一索引，主键索引)还是要读磁盘的。 
2. 减少内存占用：change buffer虽然还是需要内存占用(记录数据更新动作)，但相比于数据页来说(默认16K)，所占的内存还是小了很多的。



**什么时候使用change buffer，以及为什么唯一索引不使用**

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。

唯一索引更新导致性能降低的原因应该是：必须把数据页加载到内存中进行判断涉及到的随机io的读写

change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 `innodb_change_buffer_max_size`来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。

**如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。**

第一种情况是，**这个记录要更新的目标页在内存中**。这时，InnoDB 的处理流程如下：

* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。

这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。

第二种情况是，**这个记录要更新的目标页不在内存中**。这时，InnoDB 的处理流程如下：

* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

某个业务的库内存命中率突然从 99% 降低到了 75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引。<!--注意这里是insert操作。insert操作必然要更新所有的索引树，当然就包括了唯一索引树，那么在更新这棵树的时候就用不上change buffer，就需要频繁访问磁盘，降低内存命中率。而update操作，只要不涉及到唯一索引字段的更新，就不会去更新唯一索引树，所以update操作的消耗是比较低的。所以涉及到有大量insert的业务场景，发生内存命中率低的问题，就需要考虑是不是索引的问题了。-->

## 9.3 change buffer 的使用场景

> change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。那么.

**现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？**

因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。

因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。

## 9.4 索引选择和实践

这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。

特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。<!--也就是读少写多的场景-->

## 9.5 什么是buffer pool

> 硬盘在读写速度上相比内存有着数量级差距，如果每次读写都要从磁盘加载相应数据页，DB的效率就上不来，因而为了化解这个困局，几乎所有的DB都会把缓存池当做标配（在内存中开辟的一整块空间，由引擎利用一些命中算法和淘汰算法负责维护和管理），change buffer则更进一步，把在内存中更新就能可以立即返回执行结果并且满足一致性约束（显式或隐式定义的约束条件）的记录也暂时放在缓存池中，这样大大减少了磁盘IO操作的几率

## 9.6 change buffer 和 redo log

理解了 change buffer 的原理，你可能会联想到我在前面文章中和你介绍过的 redo log 和 WAL。

> redo log 与 change buffer(含磁盘持久化) 这2个机制，不同之处在于优化了整个变更流程的不同阶段。 先不考虑redo log、change buffer机制，简化抽象一个变更(insert、update、delete)流程： 
>
> 1、从磁盘读取待变更的行所在的数据页，读取至内存页中。
>
>  2、对内存页中的行，执行变更操作 
>
> 3、将变更后的数据页，写入至磁盘中。
>
>  步骤1，涉及 随机 读磁盘IO； 步骤3，涉及 随机 写磁盘IO；
>
>  --change buffer机制，优化了步骤1,避免了随机读磁盘IO; 
>
> --redo log机制， 优化了步骤3,避免了随机写磁盘IO，将随机写磁盘，优化为了顺序写磁盘(写redo log，确保crash-safe) ；
>
>  --在我们mysql innodb中， change buffer机制不是一直会被应用到，仅当待操作的数据页当前不在内存中，需要先读磁盘加载数据页时，change buffer才有用武之地。 redo log机制，为了保证crash-safe，一直都会用到。

**对于以下语句分析执行流程**

```mysql
insert into t(id,k) values(id1,k1),(id2,k2);
```

这里，我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图。

![change buffer 的更新过程](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111281556442.png)

* 内存、
* redo log（ib_log_fileX）、
*  数据表空间（t.ibd）、数据表空间：就是一个个的表数据文件，对应的磁盘文件就是“表名.ibd”；
* 系统表空间（ibdata1）。系统表空间：用来放系统信息，如数据字典等，对应的磁盘文件是“ibdata1”

这条更新语句做了如下的操作（按照图中的数字顺序）：

1. Page 1 在内存中，直接更新内存；

2. Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”

3. 这个信息将上述两个动作记入 redo log 中（图中 3 和 4）。

做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。

**那么在这之后新来了一个select请求如何处理**

比如，我们现在要执行 select * from t where k in (k1, k2)。这里，我画了这两个读请求的流程图。

如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，我在图中就没画出这两部分。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111281600997.png)

从图中可以看到：

1. 读 Page 1 的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。
2. 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。<!--merge的时机应该是读取数据页到内存中的时候，这时会先在内存的change buffer找有没有修改，没找到继续到磁盘的change buffer找，如果都没有找到证明这个数据页没有修改。
   change buffer内容会通过redo log刷到磁盘页吗？不会，是通过内存/磁盘中的change buffer刷的。-->

所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**

## 小结

redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成；redo日志记录的是change buffer页的变化。 change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd

唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。

1. changebuffer跟普通数据页一样也是存在磁盘里，区别在于changebuffer是在共享表空间ibdata1里
2. redolog有两种，一种记录普通数据页的改动，一种记录changebuffer的改动
3. 只要内存里脏页（innodb buffer pool）里的数据发生了变化，就一定会记录2中前一种redolog
   （对数据的修改记录在changebuffer里的时候，内存里是没有这个物理页的，不存在脏页）
4. 真正对磁盘数据页的修改是通过将内存里脏页的数据刷回磁盘来完成的，而不是根据redolog



change Buffer和数据页一样，也是物理页的一个组成部分，数据结构也是一颗B+树，这棵B+树放在共享表空间中，默认ibdata1中。change buffer 写入系统表空间机制应该和普通表的脏页刷新到磁盘是相同的机制--Checkpoint机制；之所以change buffer要写入系统表空间，是为了保证数据的一致性，change buffer做修改时需要写redo log，在做恢复时需要根据redo log来恢复change buffer，若是不进行change buffer写入系统表空间，也就是不进行持久化，那么在change buffer写入内存后掉电（也就是篇尾提出的问题），则无法进行数据恢复。这样也会导致索引中的数据和相应表的相应列中的数据不一致。change buffer 写入到了系统表空间，merge 的时候会先查询change buffer里对应的记录，然后进行purge，因为change buffer B+树的key是表空间ID，所以查询根据表空间ID 查询change buffer会很快。



**注意**

> redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成；redo日志记录的是change buffer页的变化。
> change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd。

首先明确一个观点，redo log最大的作用，就是用于数据库异常宕机的恢复工作。
如果数据库永远不会宕机，那么不需要 redo log。redo log 和 change buffer其实关注的是两个事情，不能混为一谈。

其次，数据库缓冲池中有如下内容：数据页，索引页，插入缓冲，等等其他页（其他页目前不需要了解），数据页可以理解为叶子节点，索引页可以理解为非叶子节点，插入缓冲就是老师这节课讲的 change buffer。

当做insert update delete操作时，会涉及到两方面的更新，一类是主键索引B+树，另一类的非主键索引B+树。针对，主键索引B+树和 非主键索引中的唯一索引B+树，如果在内存中，直接更新内存；如果不在内存，直接从数据库中读取页到内存中来，更新内存即可。针对非主键索引的普通索引B+树，如果树在内存中，直接更新内存；如果不在内存中，更新change buffer，等到后面需要使用这个树的时候，会从磁盘中读取，然后做merge操作。

有同学问，到底是依据change buffer磁盘还是依据redo log更新磁盘，我的回答是，他们都不会直接更新磁盘，刷新磁盘的工作是innodb存储引擎中的线程去做的。redo log负责的是 异常宕机的恢复；change buffer用于 提高普通索引更新的性能。

![img](https://raw.githubusercontent.com/lijinzedev/picture/main/img/202111282128560.png)

## FAQ

**你可以看到，change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？**



---

**如果是针对非唯一索引和唯一索引的更新和delete而且条件是where 索引值=这种情况,是否二级索引和唯一索引就没有区别呢**

这时候要“先读后写”，读的时候数据会读入内存，更新的时候直接改内存，就不需要change buffer了

change buffer就是为了延迟更新数据的时候对二级索引的更新，而where 索引值=，就是用的二级索引来更新的，更新之前得先把二级索引的树读出来，既然已经读出来了，就可以直接更新了，没必要用change buffer了

**如何判断内存命中率**

Hit rate

**change buffer相当于推迟了更新操作，那对并发控制相关的是否有影响，比如加锁？我一直以为加锁需要把具体的数据页读到内存中来，才能加锁，然而并不是？**

锁是一个单独的数据结构，如果数据页上有锁，change buffer 在判断“是否能用”的时候，就会认为否

change Buffer 针对的对象是索引页，锁是加在数据页上的吧。如果更新的时候发现数据页已经有锁了，change Buffer自然会失效 

**Merge 行为之后还会产生redo log吗？**

第一步，merge其实是从磁盘读数据页到内存，然后应用，这一步都是更新的内存，同时写redolog

现在内存变成脏页了，跟磁盘数据不一样。之后就走刷脏页的流程。刷脏页也不用写。

个人理解会产生的因为第一次redo log 记录的是change buffer对应的页，而merge过程记录的是内存页

**为什么change buffer已经使用了redo log 还需要持久化**

猜想大概是因为change buffer 需要快速的检束出数据以便于进行merge操作，所以必须要保持B+树这种高效的结构，再者就是redo log 的空间有限，不能存很多数据

当数据库崩溃时可以通过redo log将change buffer内容回放出来。” 是的，所以change buffer其实也是用了WAL机制。

**change buffer中存的内容是“在某个数据页更新什么”，但是在update/insert时，确定这条记录更新/插入在哪个数据页，不也是有一个查找的过程么？（肯定有一个一层层查找的过程，会路过很多数据页啊）为了确定在哪个数据页操作而遍历过的数据页也会读进内存作缓存吗？**

是的，查找过程避不开，但是二级索引树的非叶子节点没多少，主要在磁盘上的还是叶子节点。

准确来说在二级索引的叶子节点的上一层节点中就可以确定要插入到哪一页中，因为在二级索引的叶子节点时索引列+主键值，他的非叶子节点中每页存储的是叶子节点中每一页中最小的那个索引列+主键的一条记录+页号，当我们插入的时候就可以从最后一层的非叶子节点中找到应该插入的页

查非叶子节点，都是在内存中进行的。查叶子节点，并且将这页载入到内存， 这个比较耗时，特别是在写多读少的场景下，性能差很多

**如果说因为内存不足需要回收change buffer这部分内存，那也应当将数据merge后刷入磁盘吧。**

“内存不足需要回收change buffer这部分内存“，只需要让change buffer本身持久化可以，不需要执行merge操作。merge操作是在读数据页的时候做的

**update操作不是先读后写吗？如果是先读的话，不是应该把数据已经读到内存了吗？那这样的话直接更新内存不就好了，为什么还要写change buffer**

 如果一个表上有字段 a, b, 且有普通索引c， update语句为 update xxx set a = 'xxx' where c = 'xxx'， 执行这个语句的时候， 因为要判断c = 'xxx' 的记录是否存在， 存在的话才会更新， 此时必定要将c 索引的内存页载入到内存中（执行计划会走索引c) ， 修改内存页，写redo log; 同时主键索引页也要载入内存进行修改（主键索引永远都需要先读后写，这个免不了）。这个就用不到Change Buffer。
但是如果是另外一种场景： update xxx set a = 'xxx' where b = 'xxx' ， 这个时候因为查询条件不走索引c，故不需要将c的数据页载入内存， 针对索引c上的修改就写入到Change Buffer， 但是针对主键索引，还是要先载入内存再修改。 此时的优化就是针对索引c的页的随机io的优化。

**关于change buffer 主要节省的则是随机读磁盘的 IO消耗这个点，我的理解是如果没有change buffer机制，那么在执行更新后（写入redolog），读取数据的时候需要从次磁盘随机读取redolog合并到数据中，主要减少的是这部分消耗？**

不是，如果没有change buffer, 执行更新的“当时那一刻”，就要求从磁盘把数据页读出来（这个操作是随机读）。

**数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。后面又说change是占用pool内存的，那到底占不占用buffer pool的内存**

1. change buffer本身是占用内存的；
2. 但是chage buffer本身只是记录了“更新过程”，远远比数据页（一个16k）小。相比于把数据页读入内存，这个方式还是省了内存的。

**change buffer的持久化是不是考虑到随着事务的运行，内存中已经存放不下change buffer了，所以才考虑要持久化到系统表空间中去的？**

change buffer的写盘策略跟数据一样，内存放不下会触发落盘，还有checkpoint推进的时候也是可能会出发



**对于二级普通索引，update t where k=？这种语句，执行的时候，是先读后写吗，也就是先按k索引树将相应索引页都加载到内存，然后再按主键索引将数据页面加载到内存？还是按照文中提到的先记录到change buffer？**

如果是where k= N 是要先读的（确保数据存在）

读的是主键b+树，带数据，但是不用读普通索引的B+树，意思就是change buffer只缓存了普通索引的B+树的操作，只有要使用这个索引的时候，这些操作才能真正生效，

**changeBuffer 更新过程**

changeBuffer详细一点来说是对普通索引页的优化。
普通索引（name） （age）主键（id）
（4，张三）
语句一:update name=李四 where id = 4
语句二:update age where name='李四'
可知主键为聚簇索引，存着整行数据。普通索引存着当前索引段的数据和主键值。
语句一:
1.主键数据（页）读到内存，修改主键上的数据行。
2.修改普通索引name上的name字段数据。
第一步是必须读盘到内存中的，然后修改对应数据,得到修改数据行。第二步是要去修改name索引的数据，这时就可以将此操作放到changeBuffer里，等待下次用到name索引时（用到就会读到内存中），再将其修改，所以就省了第二步的读盘操作。所以普通索引的name暂时是张三。
语句二（紧接着语句一）:
1.name索引数据读到内存，根据hash发现changBuffer上有本次读取页的数据没有改，就顺便改一下。
2.索引到name=李四的id,然后去修改主键中的age数据，修改完主键的后，这时就该去修改age索引的数据了，但age是普通索引，所以会将操作放到changeBuffer里，等待时机去更新。

**在不读磁盘的情况下，delete/update影响行数如何获取？**

不可能完全不读盘的，只是说可能某些二级索引可以不用读盘，不管做的是insert, update还是delete操作，都涉及到主键索引上数据的更新，主键索引的相关页都是需要载入内存的，通过主键索引的变化就可以获取影响行数
